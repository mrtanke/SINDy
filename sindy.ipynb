{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fc448f1",
   "metadata": {},
   "source": [
    "# SINDy: Sparse Identification of Nonlinear Dynamics\n",
    "\n",
    "This notebook implements symbolic regression using the SINDy algorithm to discover governing equations from data. We explore:\n",
    "\n",
    "1. **SINDy in Ground Truth Coordinates** - Direct application on pendulum dynamics\n",
    "2. **SINDy-Autoencoder** - Learning from Cartesian coordinates when true coordinates are unknown\n",
    "3. **Bonus: SINDy on Videos** - High-dimensional video data\n",
    "\n",
    "## Mathematical Background\n",
    "\n",
    "The pendulum equation: $\\ddot{z}_t = -\\sin(z_t)$\n",
    "\n",
    "SINDy expresses the ODE as: $\\ddot{z}_t = \\Theta(z_t, \\dot{z}_t) \\cdot \\Xi^*$\n",
    "\n",
    "where $\\Theta$ is a library of candidate functions and $\\Xi^*$ contains the sparse coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83732bb6",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520414bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from scipy.integrate import odeint\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from IPython.display import HTML\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95fa7a4",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 1: SINDy in Ground Truth Coordinates $z$\n",
    "\n",
    "## 1.1 Simulation\n",
    "\n",
    "The pendulum's equation of motion is:\n",
    "$$\\ddot{z}_t = -\\sin(z_t)$$\n",
    "\n",
    "For SINDy, we express this as:\n",
    "$$\\ddot{z}_t = \\Theta(z_t, \\dot{z}_t) \\cdot \\Xi^* = \\sin(z_t) \\cdot (-1.0)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d3d1a0",
   "metadata": {},
   "source": [
    "## 1.2 Implementation & Training\n",
    "\n",
    "### Define the SINDy Library\n",
    "\n",
    "The library of candidate functions:\n",
    "$$\\Theta(z, \\dot{z}) = [1, z, \\dot{z}, \\sin(z), z^2, z \\cdot \\dot{z}, \\dot{z} \\cdot \\sin(z), \\dot{z}^2, \\dot{z} \\cdot \\sin(z), \\sin(z)^2]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9673fd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the library of candidate functions\n",
    "def get_library_terms():\n",
    "    \"\"\"\n",
    "    Returns a list of functions representing the SINDy library terms.\n",
    "    Each function takes (z, dz) and returns the term value.\n",
    "    \n",
    "    Library: [1, z, dz, sin(z), z², z·dz, dz·sin(z), dz², dz·sin(z), sin(z)²]\n",
    "    Note: The exercise lists dz·sin(z) twice, we'll keep 10 terms as specified.\n",
    "    \"\"\"\n",
    "    terms = [\n",
    "        lambda z, dz: np.ones_like(z),           # 1\n",
    "        lambda z, dz: z,                          # z\n",
    "        lambda z, dz: dz,                         # dz\n",
    "        lambda z, dz: np.sin(z),                  # sin(z)\n",
    "        lambda z, dz: z**2,                       # z²\n",
    "        lambda z, dz: z * dz,                     # z·dz\n",
    "        lambda z, dz: dz * np.sin(z),             # dz·sin(z)\n",
    "        lambda z, dz: dz**2,                      # dz²\n",
    "        lambda z, dz: dz * np.sin(z),             # dz·sin(z) (duplicate as per spec)\n",
    "        lambda z, dz: np.sin(z)**2,               # sin(z)²\n",
    "    ]\n",
    "    return terms\n",
    "\n",
    "def get_term_names():\n",
    "    \"\"\"Returns human-readable names for each library term.\"\"\"\n",
    "    return ['1', 'z', 'dz', 'sin(z)', 'z²', 'z·dz', 'dz·sin(z)', 'dz²', 'dz·sin(z)', 'sin(z)²']\n",
    "\n",
    "# Ground truth coefficients: only sin(z) term has coefficient -1\n",
    "GROUND_TRUTH_COEFFICIENTS = np.array([0., 0., 0., -1., 0., 0., 0., 0., 0., 0.])\n",
    "\n",
    "print(\"Library terms:\", get_term_names())\n",
    "print(\"Ground truth coefficients:\", GROUND_TRUTH_COEFFICIENTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed17cb7c",
   "metadata": {},
   "source": [
    "### Pendulum ODE Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4551910",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pendulum_rhs(zt, dzt, coefficients, terms):\n",
    "    \"\"\"\n",
    "    Compute scalar product Θ(z, dz) · Ξ at given time points.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    zt : array-like\n",
    "        Angle values (can be vector)\n",
    "    dzt : array-like\n",
    "        Angular velocity values (can be vector)\n",
    "    coefficients : array-like\n",
    "        Coefficient vector Ξ\n",
    "    terms : list\n",
    "        List of library term functions\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    ddzt : array-like\n",
    "        Second derivative (acceleration)\n",
    "    \"\"\"\n",
    "    zt = np.atleast_1d(zt)\n",
    "    dzt = np.atleast_1d(dzt)\n",
    "    \n",
    "    # Build library matrix Θ\n",
    "    theta = np.column_stack([term(zt, dzt) for term in terms])\n",
    "    \n",
    "    # Compute Θ · Ξ\n",
    "    ddzt = theta @ coefficients\n",
    "    \n",
    "    return ddzt\n",
    "\n",
    "\n",
    "def pendulum_ode_step(y, t, coefficients, terms):\n",
    "    \"\"\"\n",
    "    Function for use with scipy.integrate.odeint.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    y : array-like\n",
    "        State vector [z, dz]\n",
    "    t : float\n",
    "        Time (not used, but required by odeint)\n",
    "    coefficients : array-like\n",
    "        Coefficient vector Ξ\n",
    "    terms : list\n",
    "        List of library term functions\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dydt : array-like\n",
    "        Derivative [dz, ddz]\n",
    "    \"\"\"\n",
    "    z, dz = y\n",
    "    ddz = pendulum_rhs(z, dz, coefficients, terms)\n",
    "    return [dz, float(ddz)]\n",
    "\n",
    "\n",
    "def simulate_pendulum(z0, dz0, coefficients, terms, T, dt):\n",
    "    \"\"\"\n",
    "    Simulate pendulum using odeint.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    z0 : float\n",
    "        Initial angle\n",
    "    dz0 : float\n",
    "        Initial angular velocity\n",
    "    coefficients : array-like\n",
    "        Coefficient vector Ξ\n",
    "    terms : list\n",
    "        List of library term functions\n",
    "    T : int\n",
    "        Number of timesteps\n",
    "    dt : float\n",
    "        Time step size\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    t : array\n",
    "        Time array\n",
    "    z : array\n",
    "        Angle trajectory\n",
    "    dz : array\n",
    "        Angular velocity trajectory\n",
    "    ddz : array\n",
    "        Angular acceleration trajectory\n",
    "    \"\"\"\n",
    "    t = np.arange(T) * dt\n",
    "    y0 = [z0, dz0]\n",
    "    \n",
    "    # Integrate ODE\n",
    "    solution = odeint(pendulum_ode_step, y0, t, args=(coefficients, terms))\n",
    "    \n",
    "    z = solution[:, 0]\n",
    "    dz = solution[:, 1]\n",
    "    \n",
    "    # Compute ddz from the equation\n",
    "    ddz = pendulum_rhs(z, dz, coefficients, terms)\n",
    "    \n",
    "    return t, z, dz, ddz\n",
    "\n",
    "\n",
    "# Test with ground truth coefficients\n",
    "terms = get_library_terms()\n",
    "t_test, z_test, dz_test, ddz_test = simulate_pendulum(\n",
    "    z0=0.5, dz0=0.0, \n",
    "    coefficients=GROUND_TRUTH_COEFFICIENTS, \n",
    "    terms=terms, \n",
    "    T=100, dt=0.02\n",
    ")\n",
    "\n",
    "print(f\"Simulation test: T={len(t_test)}, z range=[{z_test.min():.3f}, {z_test.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff20ff7",
   "metadata": {},
   "source": [
    "### Create Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1a0ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pendulum_data(z0_min, z0_max, dz0_min, dz0_max, coefficients, terms, \n",
    "                         T, dt, N, embedding=None, rejection=True):\n",
    "    \"\"\"\n",
    "    Create training set of N simulations from uniform random initial conditions.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    z0_min, z0_max : float\n",
    "        Range for initial angle\n",
    "    dz0_min, dz0_max : float\n",
    "        Range for initial angular velocity\n",
    "    coefficients : array-like\n",
    "        Coefficient vector Ξ\n",
    "    terms : list\n",
    "        List of library term functions\n",
    "    T : int\n",
    "        Number of timesteps per simulation\n",
    "    dt : float\n",
    "        Time step size\n",
    "    N : int\n",
    "        Number of simulations\n",
    "    embedding : str or None\n",
    "        'cartesian', 'grid', or None for raw coordinates\n",
    "    rejection : bool\n",
    "        If True, reject high angular momentum initial conditions\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    data : dict\n",
    "        Dictionary containing z, dz, ddz, t, and optionally embedded coordinates\n",
    "    \"\"\"\n",
    "    all_z = []\n",
    "    all_dz = []\n",
    "    all_ddz = []\n",
    "    all_t = []\n",
    "    \n",
    "    n_accepted = 0\n",
    "    n_rejected = 0\n",
    "    \n",
    "    while n_accepted < N:\n",
    "        # Sample random initial conditions\n",
    "        z0 = np.random.uniform(z0_min, z0_max)\n",
    "        dz0 = np.random.uniform(dz0_min, dz0_max)\n",
    "        \n",
    "        # Check rejection criterion: |½dz0² - cos(z0)| > 0.99\n",
    "        if rejection:\n",
    "            energy = np.abs(0.5 * dz0**2 - np.cos(z0))\n",
    "            if energy > 0.99:\n",
    "                n_rejected += 1\n",
    "                continue\n",
    "        \n",
    "        # Simulate\n",
    "        t, z, dz, ddz = simulate_pendulum(z0, dz0, coefficients, terms, T, dt)\n",
    "        \n",
    "        all_z.append(z)\n",
    "        all_dz.append(dz)\n",
    "        all_ddz.append(ddz)\n",
    "        all_t.append(t)\n",
    "        n_accepted += 1\n",
    "    \n",
    "    if rejection and n_rejected > 0:\n",
    "        print(f\"Rejection sampling: accepted {n_accepted}, rejected {n_rejected}\")\n",
    "    \n",
    "    # Stack into arrays\n",
    "    z_arr = np.array(all_z)      # Shape: (N, T)\n",
    "    dz_arr = np.array(all_dz)\n",
    "    ddz_arr = np.array(all_ddz)\n",
    "    t_arr = np.array(all_t)\n",
    "    \n",
    "    data = {\n",
    "        'z': z_arr,\n",
    "        'dz': dz_arr,\n",
    "        'ddz': ddz_arr,\n",
    "        't': t_arr,\n",
    "        'N': N,\n",
    "        'T': T,\n",
    "        'dt': dt\n",
    "    }\n",
    "    \n",
    "    # Apply embedding if specified\n",
    "    if embedding == 'cartesian':\n",
    "        x, dx, ddx = embed_cartesian(z_arr, dz_arr, ddz_arr)\n",
    "        data['x'] = x\n",
    "        data['dx'] = dx\n",
    "        data['ddx'] = ddx\n",
    "    elif embedding == 'grid':\n",
    "        # Will be implemented later\n",
    "        pass\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "# Create training data with recommended parameters\n",
    "N = 100\n",
    "z0_min, z0_max = -np.pi, np.pi\n",
    "dz0_min, dz0_max = -2.1, 2.1\n",
    "T = 50\n",
    "dt = 0.02\n",
    "\n",
    "terms = get_library_terms()\n",
    "\n",
    "print(\"Creating training data...\")\n",
    "train_data = create_pendulum_data(\n",
    "    z0_min=z0_min, z0_max=z0_max,\n",
    "    dz0_min=dz0_min, dz0_max=dz0_max,\n",
    "    coefficients=GROUND_TRUTH_COEFFICIENTS,\n",
    "    terms=terms,\n",
    "    T=T, dt=dt, N=N,\n",
    "    embedding=None,\n",
    "    rejection=True\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining set size: {N} simulations × {T} timesteps = {N*T} samples\")\n",
    "print(f\"z shape: {train_data['z'].shape}\")\n",
    "print(f\"dz shape: {train_data['dz'].shape}\")\n",
    "print(f\"ddz shape: {train_data['ddz'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf22788",
   "metadata": {},
   "source": [
    "### Visualize Simulated Data\n",
    "\n",
    "Select 5 simulations and plot $z_t$, $\\dot{z}_t$, $\\ddot{z}_t$ versus time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ce862c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize 5 random simulations\n",
    "fig, axes = plt.subplots(3, 5, figsize=(15, 8))\n",
    "\n",
    "selected_indices = np.random.choice(N, 5, replace=False)\n",
    "\n",
    "for col, idx in enumerate(selected_indices):\n",
    "    t = train_data['t'][idx]\n",
    "    z = train_data['z'][idx]\n",
    "    dz = train_data['dz'][idx]\n",
    "    ddz = train_data['ddz'][idx]\n",
    "    \n",
    "    axes[0, col].plot(t, z, 'b-')\n",
    "    axes[0, col].set_ylabel('$z_t$' if col == 0 else '')\n",
    "    axes[0, col].set_title(f'Simulation {idx}')\n",
    "    axes[0, col].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[1, col].plot(t, dz, 'g-')\n",
    "    axes[1, col].set_ylabel('$\\dot{z}_t$' if col == 0 else '')\n",
    "    axes[1, col].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[2, col].plot(t, ddz, 'r-')\n",
    "    axes[2, col].set_ylabel('$\\ddot{z}_t$' if col == 0 else '')\n",
    "    axes[2, col].set_xlabel('Time (s)')\n",
    "    axes[2, col].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Pendulum Dynamics: $z$, $\\dot{z}$, $\\ddot{z}$ vs Time', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Curves should oscillate and be shifted relative to each other.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882009e9",
   "metadata": {},
   "source": [
    "### Optional: Animate Pendulum\n",
    "\n",
    "Animate pendulum motion with tip coordinates $x_1(t) = \\sin(z_t)$, $x_2(t) = -\\cos(z_t)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d41fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def animate_pendulum(z, dt, save_path=None):\n",
    "    \"\"\"\n",
    "    Animate pendulum motion.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    z : array\n",
    "        Angle trajectory\n",
    "    dt : float\n",
    "        Time step\n",
    "    save_path : str or None\n",
    "        Path to save animation\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    anim : FuncAnimation object\n",
    "    \"\"\"\n",
    "    # Compute tip positions\n",
    "    x1 = np.sin(z)\n",
    "    x2 = -np.cos(z)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    ax.set_xlim(-1.5, 1.5)\n",
    "    ax.set_ylim(-1.5, 1.5)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_xlabel('$x_1$')\n",
    "    ax.set_ylabel('$x_2$')\n",
    "    ax.set_title('Pendulum Animation')\n",
    "    \n",
    "    # Draw pivot\n",
    "    pivot, = ax.plot([0], [0], 'ko', markersize=10)\n",
    "    \n",
    "    # Initialize line and bob\n",
    "    line, = ax.plot([], [], 'b-', linewidth=2)\n",
    "    bob, = ax.plot([], [], 'ro', markersize=15)\n",
    "    trail, = ax.plot([], [], 'r-', alpha=0.3, linewidth=1)\n",
    "    \n",
    "    trail_x, trail_y = [], []\n",
    "    \n",
    "    def init():\n",
    "        line.set_data([], [])\n",
    "        bob.set_data([], [])\n",
    "        trail.set_data([], [])\n",
    "        return line, bob, trail\n",
    "    \n",
    "    def animate(i):\n",
    "        line.set_data([0, x1[i]], [0, x2[i]])\n",
    "        bob.set_data([x1[i]], [x2[i]])\n",
    "        \n",
    "        trail_x.append(x1[i])\n",
    "        trail_y.append(x2[i])\n",
    "        trail.set_data(trail_x, trail_y)\n",
    "        \n",
    "        return line, bob, trail\n",
    "    \n",
    "    anim = FuncAnimation(fig, animate, init_func=init, \n",
    "                        frames=len(z), interval=dt*1000, blit=True)\n",
    "    \n",
    "    plt.close(fig)\n",
    "    return anim\n",
    "\n",
    "\n",
    "# Create animation for one simulation\n",
    "anim = animate_pendulum(train_data['z'][0], dt)\n",
    "HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d03162",
   "metadata": {},
   "source": [
    "### SINDy with Sklearn LASSO\n",
    "\n",
    "Implement LASSO regression to find sparse coefficients:\n",
    "$$\\hat{\\Xi} = \\arg\\min_{\\Xi}\\left[\\frac{1}{T \\cdot N}\\sum_{i=1}^{T \\cdot N}\\left|\\ddot{z}^i - \\Theta(z^i, \\dot{z}^i) \\cdot \\Xi\\right|_2^2 + \\lambda |\\Xi|_1\\right]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a555f6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_library_matrix(z, dz, terms):\n",
    "    \"\"\"\n",
    "    Build the library matrix Θ(z, dz).\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    z : array-like\n",
    "        Flattened angle values\n",
    "    dz : array-like\n",
    "        Flattened angular velocity values\n",
    "    terms : list\n",
    "        List of library term functions\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    Theta : ndarray\n",
    "        Library matrix of shape (n_samples, n_terms)\n",
    "    \"\"\"\n",
    "    z = np.atleast_1d(z).flatten()\n",
    "    dz = np.atleast_1d(dz).flatten()\n",
    "    \n",
    "    Theta = np.column_stack([term(z, dz) for term in terms])\n",
    "    return Theta\n",
    "\n",
    "\n",
    "def sindy_sklearn(z, dz, ddz, terms, alpha=1e-5):\n",
    "    \"\"\"\n",
    "    Train SINDy using sklearn LASSO.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    z, dz, ddz : ndarray\n",
    "        Training data (can be 2D with shape (N, T))\n",
    "    terms : list\n",
    "        List of library term functions\n",
    "    alpha : float\n",
    "        L1 regularization strength\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    coefficients : ndarray\n",
    "        Learned coefficient vector\n",
    "    model : Lasso\n",
    "        Trained sklearn model\n",
    "    \"\"\"\n",
    "    # Flatten arrays\n",
    "    z_flat = z.flatten()\n",
    "    dz_flat = dz.flatten()\n",
    "    ddz_flat = ddz.flatten()\n",
    "    \n",
    "    # Build library matrix\n",
    "    Theta = build_library_matrix(z_flat, dz_flat, terms)\n",
    "    \n",
    "    # Fit LASSO\n",
    "    model = Lasso(alpha=alpha, fit_intercept=False, max_iter=10000)\n",
    "    model.fit(Theta, ddz_flat)\n",
    "    \n",
    "    return model.coef_, model\n",
    "\n",
    "\n",
    "# Train sklearn SINDy\n",
    "terms = get_library_terms()\n",
    "term_names = get_term_names()\n",
    "\n",
    "sklearn_coeffs, sklearn_model = sindy_sklearn(\n",
    "    train_data['z'], train_data['dz'], train_data['ddz'], \n",
    "    terms, alpha=1e-5\n",
    ")\n",
    "\n",
    "print(\"Sklearn LASSO Results:\")\n",
    "print(\"-\" * 40)\n",
    "for name, coef, gt in zip(term_names, sklearn_coeffs, GROUND_TRUTH_COEFFICIENTS):\n",
    "    marker = \"✓\" if np.abs(coef - gt) < 0.1 else \"\"\n",
    "    print(f\"{name:15s}: {coef:10.6f} (GT: {gt:6.2f}) {marker}\")\n",
    "\n",
    "print(f\"\\nMSE: {np.mean((sklearn_coeffs - GROUND_TRUTH_COEFFICIENTS)**2):.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e086a3ef",
   "metadata": {},
   "source": [
    "### SINDy with PyTorch\n",
    "\n",
    "Implement SINDy as a PyTorch module with learnable coefficients and boolean mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1983e095",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_library_matrix_torch(z, dz):\n",
    "    \"\"\"\n",
    "    Build the library matrix Θ(z, dz) using PyTorch tensors.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    z : torch.Tensor\n",
    "        Angle values\n",
    "    dz : torch.Tensor\n",
    "        Angular velocity values\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    Theta : torch.Tensor\n",
    "        Library matrix of shape (n_samples, n_terms)\n",
    "    \"\"\"\n",
    "    z = z.view(-1)\n",
    "    dz = dz.view(-1)\n",
    "    \n",
    "    sin_z = torch.sin(z)\n",
    "    \n",
    "    Theta = torch.stack([\n",
    "        torch.ones_like(z),      # 1\n",
    "        z,                        # z\n",
    "        dz,                       # dz\n",
    "        sin_z,                    # sin(z)\n",
    "        z**2,                     # z²\n",
    "        z * dz,                   # z·dz\n",
    "        dz * sin_z,               # dz·sin(z)\n",
    "        dz**2,                    # dz²\n",
    "        dz * sin_z,               # dz·sin(z)\n",
    "        sin_z**2,                 # sin(z)²\n",
    "    ], dim=1)\n",
    "    \n",
    "    return Theta\n",
    "\n",
    "\n",
    "class SINDy(nn.Module):\n",
    "    \"\"\"\n",
    "    SINDy module with learnable coefficients and boolean mask.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_terms=10):\n",
    "        super(SINDy, self).__init__()\n",
    "        \n",
    "        # Learnable coefficients\n",
    "        self.coefficients = nn.Parameter(torch.ones(n_terms))\n",
    "        \n",
    "        # Boolean mask (not a parameter, but tracked)\n",
    "        self.register_buffer('mask', torch.ones(n_terms, dtype=torch.bool))\n",
    "        \n",
    "        self.n_terms = n_terms\n",
    "    \n",
    "    def forward(self, z, dz):\n",
    "        \"\"\"\n",
    "        Compute RHS: Θ(z, dz) · (Ξ ⊙ Υ)\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        z : torch.Tensor\n",
    "            Angle values\n",
    "        dz : torch.Tensor\n",
    "            Angular velocity values\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        ddz : torch.Tensor\n",
    "            Predicted second derivative\n",
    "        \"\"\"\n",
    "        Theta = build_library_matrix_torch(z, dz)\n",
    "        \n",
    "        # Apply mask to coefficients\n",
    "        masked_coeffs = self.coefficients * self.mask.float()\n",
    "        \n",
    "        # Compute Θ · Ξ\n",
    "        ddz = Theta @ masked_coeffs\n",
    "        \n",
    "        return ddz\n",
    "    \n",
    "    def get_coefficients(self):\n",
    "        \"\"\"Return masked coefficients as numpy array.\"\"\"\n",
    "        return (self.coefficients * self.mask.float()).detach().cpu().numpy()\n",
    "    \n",
    "    def get_mask(self):\n",
    "        \"\"\"Return mask as numpy array.\"\"\"\n",
    "        return self.mask.detach().cpu().numpy()\n",
    "    \n",
    "    def set_mask(self, mask):\n",
    "        \"\"\"Set the mask.\"\"\"\n",
    "        self.mask.copy_(torch.tensor(mask, dtype=torch.bool))\n",
    "    \n",
    "    def l1_regularization(self):\n",
    "        \"\"\"Compute L1 norm of masked coefficients.\"\"\"\n",
    "        return torch.sum(torch.abs(self.coefficients * self.mask.float()))\n",
    "\n",
    "\n",
    "# Test SINDy module\n",
    "sindy_model = SINDy(n_terms=10).to(device)\n",
    "z_test = torch.randn(100).to(device)\n",
    "dz_test = torch.randn(100).to(device)\n",
    "ddz_pred = sindy_model(z_test, dz_test)\n",
    "print(f\"SINDy model test: input shape {z_test.shape}, output shape {ddz_pred.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f96c16",
   "metadata": {},
   "source": [
    "### Training Function with Thresholding\n",
    "\n",
    "Implement `train_sindy()` with optional Sequential Thresholding (ST) and Patient Trend-Aware Thresholding (PTAT)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db23df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sindy(model, train_loader, val_loader, \n",
    "                n_epochs=1000, lr=1e-3, lambda_l1=1e-5,\n",
    "                thresholding=None, threshold_a=0.1, threshold_b=0.002,\n",
    "                threshold_interval=500, patience=1000,\n",
    "                verbose=True):\n",
    "    \"\"\"\n",
    "    Train SINDy model with optional thresholding.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : SINDy\n",
    "        SINDy model to train\n",
    "    train_loader : DataLoader\n",
    "        Training data loader\n",
    "    val_loader : DataLoader\n",
    "        Validation data loader\n",
    "    n_epochs : int\n",
    "        Number of training epochs\n",
    "    lr : float\n",
    "        Learning rate\n",
    "    lambda_l1 : float\n",
    "        L1 regularization strength\n",
    "    thresholding : str or None\n",
    "        'sequential', 'patient', or None\n",
    "    threshold_a : float\n",
    "        Coefficient magnitude threshold\n",
    "    threshold_b : float\n",
    "        Coefficient change threshold (for PTAT)\n",
    "    threshold_interval : int\n",
    "        Epochs between thresholding (for ST)\n",
    "    patience : int\n",
    "        Patience for PTAT\n",
    "    verbose : bool\n",
    "        Print progress\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    history : dict\n",
    "        Training history with losses and coefficients\n",
    "    \"\"\"\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    mse_loss = nn.MSELoss()\n",
    "    \n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'coefficients': [],\n",
    "        'mask': []\n",
    "    }\n",
    "    \n",
    "    # For PTAT\n",
    "    if thresholding == 'patient':\n",
    "        xi_prev = torch.zeros_like(model.coefficients)\n",
    "        E_a = torch.zeros(model.n_terms, device=device)  # Last epoch where |Ξ| > a\n",
    "        E_b = torch.zeros(model.n_terms, device=device)  # Last epoch where |Ξ - Ξ_prev| > b\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        \n",
    "        for z_batch, dz_batch, ddz_batch in train_loader:\n",
    "            z_batch = z_batch.to(device)\n",
    "            dz_batch = dz_batch.to(device)\n",
    "            ddz_batch = ddz_batch.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            ddz_pred = model(z_batch, dz_batch)\n",
    "            loss = mse_loss(ddz_pred, ddz_batch.view(-1))\n",
    "            \n",
    "            # Add L1 regularization\n",
    "            loss += lambda_l1 * model.l1_regularization()\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_losses.append(loss.item())\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for z_batch, dz_batch, ddz_batch in val_loader:\n",
    "                z_batch = z_batch.to(device)\n",
    "                dz_batch = dz_batch.to(device)\n",
    "                ddz_batch = ddz_batch.to(device)\n",
    "                \n",
    "                ddz_pred = model(z_batch, dz_batch)\n",
    "                loss = mse_loss(ddz_pred, ddz_batch.view(-1))\n",
    "                val_losses.append(loss.item())\n",
    "        \n",
    "        avg_train_loss = np.mean(train_losses)\n",
    "        avg_val_loss = np.mean(val_losses)\n",
    "        \n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['val_loss'].append(avg_val_loss)\n",
    "        history['coefficients'].append(model.get_coefficients().copy())\n",
    "        history['mask'].append(model.get_mask().copy())\n",
    "        \n",
    "        # Apply thresholding\n",
    "        if thresholding == 'sequential':\n",
    "            # Sequential Thresholding (ST)\n",
    "            if (epoch + 1) % threshold_interval == 0:\n",
    "                with torch.no_grad():\n",
    "                    coeffs = model.coefficients.abs()\n",
    "                    mask = model.mask.clone()\n",
    "                    \n",
    "                    # Set small coefficients to 0 and update mask\n",
    "                    small_coeffs = coeffs < threshold_a\n",
    "                    model.coefficients.data[small_coeffs] = 0\n",
    "                    mask[small_coeffs] = False\n",
    "                    model.mask.copy_(mask)\n",
    "                    \n",
    "                    if verbose:\n",
    "                        n_active = mask.sum().item()\n",
    "                        print(f\"Epoch {epoch+1}: Thresholded, {n_active} active terms\")\n",
    "        \n",
    "        elif thresholding == 'patient':\n",
    "            # Patient Trend-Aware Thresholding (PTAT)\n",
    "            with torch.no_grad():\n",
    "                coeffs = model.coefficients.clone()\n",
    "                \n",
    "                # Update E_a: epochs where |Ξ| > a\n",
    "                over_a = coeffs.abs() > threshold_a\n",
    "                E_a[over_a] = epoch + 1\n",
    "                \n",
    "                # Update E_b: epochs where |Ξ - Ξ_prev| > b\n",
    "                delta = (coeffs - xi_prev).abs() > threshold_b\n",
    "                E_b[delta] = epoch + 1\n",
    "                \n",
    "                # Compute temporary mask\n",
    "                within_patience_a = (epoch + 1 - E_a) < patience\n",
    "                within_patience_b = (epoch + 1 - E_b) < patience\n",
    "                temp_mask = within_patience_a | within_patience_b\n",
    "                \n",
    "                # Update mask (can only turn off, not on)\n",
    "                new_mask = model.mask & temp_mask\n",
    "                model.mask.copy_(new_mask)\n",
    "                \n",
    "                # Zero out masked coefficients\n",
    "                model.coefficients.data[~model.mask] = 0\n",
    "                \n",
    "                xi_prev = coeffs.clone()\n",
    "        \n",
    "        # Print progress\n",
    "        if verbose and (epoch + 1) % 100 == 0:\n",
    "            n_active = model.mask.sum().item()\n",
    "            print(f\"Epoch {epoch+1}/{n_epochs}: Train Loss = {avg_train_loss:.6f}, \"\n",
    "                  f\"Val Loss = {avg_val_loss:.6f}, Active terms = {n_active}\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "\n",
    "# Prepare data loaders\n",
    "z_flat = train_data['z'].flatten()\n",
    "dz_flat = train_data['dz'].flatten()\n",
    "ddz_flat = train_data['ddz'].flatten()\n",
    "\n",
    "# Split into train/val\n",
    "z_train, z_val, dz_train, dz_val, ddz_train, ddz_val = train_test_split(\n",
    "    z_flat, dz_flat, ddz_flat, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Create tensors\n",
    "train_dataset = TensorDataset(\n",
    "    torch.tensor(z_train, dtype=torch.float32),\n",
    "    torch.tensor(dz_train, dtype=torch.float32),\n",
    "    torch.tensor(ddz_train, dtype=torch.float32)\n",
    ")\n",
    "val_dataset = TensorDataset(\n",
    "    torch.tensor(z_val, dtype=torch.float32),\n",
    "    torch.tensor(dz_val, dtype=torch.float32),\n",
    "    torch.tensor(ddz_val, dtype=torch.float32)\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=256, shuffle=False)\n",
    "\n",
    "print(f\"Train samples: {len(train_dataset)}, Val samples: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67e5793",
   "metadata": {},
   "source": [
    "### Train PyTorch SINDy (No Thresholding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a421ab17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train PyTorch SINDy without thresholding\n",
    "print(\"Training PyTorch SINDy (no thresholding)...\")\n",
    "pytorch_sindy = SINDy(n_terms=10).to(device)\n",
    "\n",
    "history_no_thresh = train_sindy(\n",
    "    pytorch_sindy, train_loader, val_loader,\n",
    "    n_epochs=1000, lr=1e-3, lambda_l1=1e-5,\n",
    "    thresholding=None,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "pytorch_coeffs = pytorch_sindy.get_coefficients()\n",
    "\n",
    "print(\"\\nPyTorch SINDy Results (No Thresholding):\")\n",
    "print(\"-\" * 40)\n",
    "for name, coef, gt in zip(term_names, pytorch_coeffs, GROUND_TRUTH_COEFFICIENTS):\n",
    "    marker = \"✓\" if np.abs(coef - gt) < 0.1 else \"\"\n",
    "    print(f\"{name:15s}: {coef:10.6f} (GT: {gt:6.2f}) {marker}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65854dfb",
   "metadata": {},
   "source": [
    "### Compare Sklearn vs PyTorch Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9a37d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare sklearn and pytorch solutions\n",
    "print(\"Comparison of Sklearn vs PyTorch Solutions:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\n{'Term':15s} {'Sklearn':>12s} {'PyTorch':>12s} {'GT':>8s}\")\n",
    "print(\"-\"*50)\n",
    "for name, sk_coef, pt_coef, gt in zip(term_names, sklearn_coeffs, pytorch_coeffs, GROUND_TRUTH_COEFFICIENTS):\n",
    "    print(f\"{name:15s} {sk_coef:12.6f} {pt_coef:12.6f} {gt:8.2f}\")\n",
    "\n",
    "diff = np.max(np.abs(sklearn_coeffs - pytorch_coeffs))\n",
    "print(f\"\\nMax difference between sklearn and pytorch: {diff:.6e}\")\n",
    "\n",
    "if diff < 0.01:\n",
    "    print(\"✓ Solutions match within numerical accuracy!\")\n",
    "else:\n",
    "    print(\"⚠ Solutions differ - this may be due to different regularization paths\")\n",
    "\n",
    "# Visualize comparison\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "x = np.arange(len(term_names))\n",
    "width = 0.25\n",
    "\n",
    "bars1 = ax.bar(x - width, sklearn_coeffs, width, label='Sklearn LASSO', alpha=0.8)\n",
    "bars2 = ax.bar(x, pytorch_coeffs, width, label='PyTorch (no thresh)', alpha=0.8)\n",
    "bars3 = ax.bar(x + width, GROUND_TRUTH_COEFFICIENTS, width, label='Ground Truth', alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Term')\n",
    "ax.set_ylabel('Coefficient')\n",
    "ax.set_title('Comparison: Sklearn vs PyTorch vs Ground Truth')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(term_names, rotation=45, ha='right')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.axhline(y=0, color='k', linestyle='-', linewidth=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674fb18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare sklearn and pytorch solutions\n",
    "print(\"Comparison: Sklearn vs PyTorch\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Term':15s} {'Sklearn':>12s} {'PyTorch':>12s} {'Ground Truth':>12s}\")\n",
    "print(\"-\" * 60)\n",
    "for name, sk_c, pt_c, gt in zip(term_names, sklearn_coeffs, pytorch_coeffs, GROUND_TRUTH_COEFFICIENTS):\n",
    "    print(f\"{name:15s} {sk_c:12.6f} {pt_c:12.6f} {gt:12.2f}\")\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(f\"Max difference between Sklearn and PyTorch: {np.max(np.abs(sklearn_coeffs - pytorch_coeffs)):.6f}\")\n",
    "\n",
    "# Plot coefficient comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "x = np.arange(len(term_names))\n",
    "width = 0.25\n",
    "\n",
    "# Coefficient comparison\n",
    "axes[0].bar(x - width, sklearn_coeffs, width, label='Sklearn', alpha=0.8)\n",
    "axes[0].bar(x, pytorch_coeffs, width, label='PyTorch', alpha=0.8)\n",
    "axes[0].bar(x + width, GROUND_TRUTH_COEFFICIENTS, width, label='Ground Truth', alpha=0.8)\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(term_names, rotation=45, ha='right')\n",
    "axes[0].set_ylabel('Coefficient Value')\n",
    "axes[0].set_title('Learned Coefficients Comparison')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].axhline(y=0, color='k', linestyle='-', linewidth=0.5)\n",
    "\n",
    "# Training history\n",
    "axes[1].semilogy(history_no_thresh['train_loss'], label='Train Loss')\n",
    "axes[1].semilogy(history_no_thresh['val_loss'], label='Val Loss')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Loss (log scale)')\n",
    "axes[1].set_title('Training History (PyTorch)')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcfd091",
   "metadata": {},
   "source": [
    "## 1.3 Thresholding\n",
    "\n",
    "### Train with Sequential Thresholding (ST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f5423c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with Sequential Thresholding\n",
    "print(\"Training PyTorch SINDy with Sequential Thresholding (ST)...\")\n",
    "pytorch_sindy_st = SINDy(n_terms=10).to(device)\n",
    "\n",
    "history_st = train_sindy(\n",
    "    pytorch_sindy_st, train_loader, val_loader,\n",
    "    n_epochs=2000, lr=1e-3, lambda_l1=1e-5,\n",
    "    thresholding='sequential',\n",
    "    threshold_a=0.1, threshold_interval=500,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "st_coeffs = pytorch_sindy_st.get_coefficients()\n",
    "st_mask = pytorch_sindy_st.get_mask()\n",
    "\n",
    "print(\"\\nSequential Thresholding Results:\")\n",
    "print(\"-\" * 50)\n",
    "for name, coef, mask, gt in zip(term_names, st_coeffs, st_mask, GROUND_TRUTH_COEFFICIENTS):\n",
    "    status = \"Active\" if mask else \"Inactive\"\n",
    "    marker = \"✓\" if np.abs(coef - gt) < 0.1 else \"\"\n",
    "    print(f\"{name:15s}: {coef:10.6f} [{status:8s}] (GT: {gt:6.2f}) {marker}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de4fad8",
   "metadata": {},
   "source": [
    "### Train with Patient Trend-Aware Thresholding (PTAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448cfdc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with Patient Trend-Aware Thresholding\n",
    "print(\"Training PyTorch SINDy with Patient Trend-Aware Thresholding (PTAT)...\")\n",
    "pytorch_sindy_ptat = SINDy(n_terms=10).to(device)\n",
    "\n",
    "history_ptat = train_sindy(\n",
    "    pytorch_sindy_ptat, train_loader, val_loader,\n",
    "    n_epochs=2000, lr=1e-3, lambda_l1=1e-5,\n",
    "    thresholding='patient',\n",
    "    threshold_a=0.1, threshold_b=0.002, patience=1000,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "ptat_coeffs = pytorch_sindy_ptat.get_coefficients()\n",
    "ptat_mask = pytorch_sindy_ptat.get_mask()\n",
    "\n",
    "print(\"\\nPatient Trend-Aware Thresholding Results:\")\n",
    "print(\"-\" * 50)\n",
    "for name, coef, mask, gt in zip(term_names, ptat_coeffs, ptat_mask, GROUND_TRUTH_COEFFICIENTS):\n",
    "    status = \"Active\" if mask else \"Inactive\"\n",
    "    marker = \"✓\" if np.abs(coef - gt) < 0.1 else \"\"\n",
    "    print(f\"{name:15s}: {coef:10.6f} [{status:8s}] (GT: {gt:6.2f}) {marker}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b0900c",
   "metadata": {},
   "source": [
    "## 1.4 Evaluation & Visualization\n",
    "\n",
    "### Visualize Coefficient History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18d9669",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_coefficient_history(history, title, term_names):\n",
    "    \"\"\"Plot coefficient evolution over training.\"\"\"\n",
    "    coeffs_history = np.array(history['coefficients'])\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Plot all coefficients\n",
    "    for i, name in enumerate(term_names):\n",
    "        axes[0].plot(coeffs_history[:, i], label=name, alpha=0.8)\n",
    "    \n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Coefficient Value')\n",
    "    axes[0].set_title(f'{title} - Coefficient Evolution')\n",
    "    axes[0].legend(loc='upper right', fontsize=8)\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    axes[0].axhline(y=-1, color='k', linestyle='--', label='Ground Truth sin(z)')\n",
    "    \n",
    "    # Plot loss\n",
    "    axes[1].semilogy(history['train_loss'], label='Train Loss')\n",
    "    axes[1].semilogy(history['val_loss'], label='Val Loss')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Loss (log scale)')\n",
    "    axes[1].set_title(f'{title} - Training Loss')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Plot histories\n",
    "plot_coefficient_history(history_no_thresh, \"No Thresholding\", term_names)\n",
    "plot_coefficient_history(history_st, \"Sequential Thresholding\", term_names)\n",
    "plot_coefficient_history(history_ptat, \"Patient Trend-Aware Thresholding\", term_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9dfae9d",
   "metadata": {},
   "source": [
    "### Resimulate with Learned Equations\n",
    "\n",
    "Test the learned equations by resimulating with test initial conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1c2923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resimulate with learned coefficients and compare to ground truth\n",
    "def evaluate_resimulation(coefficients, terms, test_z0, test_dz0, T, dt):\n",
    "    \"\"\"\n",
    "    Resimulate with learned coefficients and compute error.\n",
    "    \"\"\"\n",
    "    # Ground truth simulation\n",
    "    _, z_gt, dz_gt, ddz_gt = simulate_pendulum(\n",
    "        test_z0, test_dz0, GROUND_TRUTH_COEFFICIENTS, terms, T, dt\n",
    "    )\n",
    "    \n",
    "    # Learned simulation\n",
    "    t, z_learned, dz_learned, ddz_learned = simulate_pendulum(\n",
    "        test_z0, test_dz0, coefficients, terms, T, dt\n",
    "    )\n",
    "    \n",
    "    # Compute errors\n",
    "    z_error = np.abs(z_learned - z_gt)\n",
    "    dz_error = np.abs(dz_learned - dz_gt)\n",
    "    \n",
    "    return t, z_gt, z_learned, z_error, dz_error\n",
    "\n",
    "\n",
    "# Test with different initial conditions\n",
    "test_initial_conditions = [\n",
    "    (0.5, 0.0),\n",
    "    (1.0, 0.5),\n",
    "    (2.0, -0.5),\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(len(test_initial_conditions), 3, figsize=(15, 4*len(test_initial_conditions)))\n",
    "\n",
    "for row, (z0, dz0) in enumerate(test_initial_conditions):\n",
    "    t, z_gt, z_st, z_err_st, _ = evaluate_resimulation(st_coeffs, terms, z0, dz0, 100, 0.02)\n",
    "    _, _, z_ptat, z_err_ptat, _ = evaluate_resimulation(ptat_coeffs, terms, z0, dz0, 100, 0.02)\n",
    "    \n",
    "    # Plot z trajectories\n",
    "    axes[row, 0].plot(t, z_gt, 'k-', label='Ground Truth', linewidth=2)\n",
    "    axes[row, 0].plot(t, z_st, 'b--', label='ST', alpha=0.8)\n",
    "    axes[row, 0].plot(t, z_ptat, 'r-.', label='PTAT', alpha=0.8)\n",
    "    axes[row, 0].set_xlabel('Time (s)')\n",
    "    axes[row, 0].set_ylabel('$z(t)$')\n",
    "    axes[row, 0].set_title(f'Trajectory: $z_0={z0}$, $\\dot{{z}}_0={dz0}$')\n",
    "    axes[row, 0].legend()\n",
    "    axes[row, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot errors\n",
    "    axes[row, 1].semilogy(t, z_err_st, 'b-', label='ST Error')\n",
    "    axes[row, 1].semilogy(t, z_err_ptat, 'r-', label='PTAT Error')\n",
    "    axes[row, 1].set_xlabel('Time (s)')\n",
    "    axes[row, 1].set_ylabel('$|z - \\hat{z}|$')\n",
    "    axes[row, 1].set_title('Error over Time')\n",
    "    axes[row, 1].legend()\n",
    "    axes[row, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot phase portrait\n",
    "    axes[row, 2].plot(z_gt, np.gradient(z_gt, t), 'k-', label='Ground Truth', linewidth=2)\n",
    "    axes[row, 2].plot(z_st, np.gradient(z_st, t), 'b--', label='ST', alpha=0.8)\n",
    "    axes[row, 2].plot(z_ptat, np.gradient(z_ptat, t), 'r-.', label='PTAT', alpha=0.8)\n",
    "    axes[row, 2].set_xlabel('$z$')\n",
    "    axes[row, 2].set_ylabel('$\\dot{z}$')\n",
    "    axes[row, 2].set_title('Phase Portrait')\n",
    "    axes[row, 2].legend()\n",
    "    axes[row, 2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compute average error over time for multiple test cases\n",
    "n_test = 20\n",
    "T_test = 50\n",
    "all_errors_st = []\n",
    "all_errors_ptat = []\n",
    "\n",
    "for _ in range(n_test):\n",
    "    z0 = np.random.uniform(-np.pi, np.pi)\n",
    "    dz0 = np.random.uniform(-2.0, 2.0)\n",
    "    \n",
    "    t, _, _, z_err_st, _ = evaluate_resimulation(st_coeffs, terms, z0, dz0, T_test, dt)\n",
    "    _, _, _, z_err_ptat, _ = evaluate_resimulation(ptat_coeffs, terms, z0, dz0, T_test, dt)\n",
    "    \n",
    "    all_errors_st.append(z_err_st)\n",
    "    all_errors_ptat.append(z_err_ptat)\n",
    "\n",
    "avg_error_st = np.mean(all_errors_st, axis=0)\n",
    "avg_error_ptat = np.mean(all_errors_ptat, axis=0)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.semilogy(t, avg_error_st, 'b-', label='ST (avg)', linewidth=2)\n",
    "plt.semilogy(t, avg_error_ptat, 'r-', label='PTAT (avg)', linewidth=2)\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Average $|z - \\hat{z}|$')\n",
    "plt.title(f'Average Error over Time ({n_test} test cases)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f485d7",
   "metadata": {},
   "source": [
    "## 1.5 Small Angle Approximation\n",
    "\n",
    "For small angles, $\\sin(z) \\approx z$. Let's train SINDy with smaller initial conditions to observe when the $z$ term becomes significant alongside or instead of $\\sin(z)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19497882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Study small angle approximation\n",
    "small_angle_ranges = [\n",
    "    (0.5, 0.5),\n",
    "    (0.3, 0.3),\n",
    "    (0.1, 0.1),\n",
    "    (0.05, 0.05),\n",
    "]\n",
    "\n",
    "results_small_angle = []\n",
    "\n",
    "for z0_max, dz0_max in small_angle_ranges:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training with z0_max = {z0_max}, dz0_max = {dz0_max}\")\n",
    "    print('='*60)\n",
    "    \n",
    "    # Create data with smaller initial conditions\n",
    "    small_data = create_pendulum_data(\n",
    "        z0_min=-z0_max, z0_max=z0_max,\n",
    "        dz0_min=-dz0_max, dz0_max=dz0_max,\n",
    "        coefficients=GROUND_TRUTH_COEFFICIENTS,\n",
    "        terms=terms, T=T, dt=dt, N=N,\n",
    "        embedding=None, rejection=False  # No rejection for small angles\n",
    "    )\n",
    "    \n",
    "    # Prepare data loaders\n",
    "    z_flat_small = small_data['z'].flatten()\n",
    "    dz_flat_small = small_data['dz'].flatten()\n",
    "    ddz_flat_small = small_data['ddz'].flatten()\n",
    "    \n",
    "    z_tr, z_vl, dz_tr, dz_vl, ddz_tr, ddz_vl = train_test_split(\n",
    "        z_flat_small, dz_flat_small, ddz_flat_small, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    train_ds = TensorDataset(\n",
    "        torch.tensor(z_tr, dtype=torch.float32),\n",
    "        torch.tensor(dz_tr, dtype=torch.float32),\n",
    "        torch.tensor(ddz_tr, dtype=torch.float32)\n",
    "    )\n",
    "    val_ds = TensorDataset(\n",
    "        torch.tensor(z_vl, dtype=torch.float32),\n",
    "        torch.tensor(dz_vl, dtype=torch.float32),\n",
    "        torch.tensor(ddz_vl, dtype=torch.float32)\n",
    "    )\n",
    "    \n",
    "    train_ld = DataLoader(train_ds, batch_size=256, shuffle=True)\n",
    "    val_ld = DataLoader(val_ds, batch_size=256, shuffle=False)\n",
    "    \n",
    "    # Train SINDy\n",
    "    model_small = SINDy(n_terms=10).to(device)\n",
    "    history_small = train_sindy(\n",
    "        model_small, train_ld, val_ld,\n",
    "        n_epochs=1000, lr=1e-3, lambda_l1=1e-5,\n",
    "        thresholding='sequential', threshold_a=0.05, threshold_interval=500,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    coeffs_small = model_small.get_coefficients()\n",
    "    mask_small = model_small.get_mask()\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\nLearned coefficients:\")\n",
    "    for name, coef, mask in zip(term_names, coeffs_small, mask_small):\n",
    "        if np.abs(coef) > 0.01:  # Only show non-negligible\n",
    "            status = \"Active\" if mask else \"Inactive\"\n",
    "            print(f\"  {name:15s}: {coef:10.6f} [{status}]\")\n",
    "    \n",
    "    results_small_angle.append({\n",
    "        'z0_max': z0_max,\n",
    "        'dz0_max': dz0_max,\n",
    "        'coefficients': coeffs_small,\n",
    "        'mask': mask_small\n",
    "    })\n",
    "\n",
    "# Visualize how coefficients change with angle magnitude\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "z_terms = [r['coefficients'][1] for r in results_small_angle]  # z term\n",
    "sin_terms = [r['coefficients'][3] for r in results_small_angle]  # sin(z) term\n",
    "angle_ranges = [r['z0_max'] for r in results_small_angle]\n",
    "\n",
    "axes[0].plot(angle_ranges, sin_terms, 'ro-', label='sin(z) coefficient', markersize=10)\n",
    "axes[0].plot(angle_ranges, z_terms, 'bs-', label='z coefficient', markersize=10)\n",
    "axes[0].axhline(y=-1, color='k', linestyle='--', label='Ground Truth')\n",
    "axes[0].set_xlabel('Max Initial Angle (rad)')\n",
    "axes[0].set_ylabel('Coefficient Value')\n",
    "axes[0].set_title('Coefficients vs Initial Angle Magnitude')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].invert_xaxis()\n",
    "\n",
    "# Compare sin(z) vs z for small angles\n",
    "z_range = np.linspace(0, 0.5, 100)\n",
    "axes[1].plot(z_range, np.sin(z_range), 'r-', label='sin(z)', linewidth=2)\n",
    "axes[1].plot(z_range, z_range, 'b--', label='z', linewidth=2)\n",
    "axes[1].plot(z_range, z_range - z_range**3/6, 'g-.', label='z - z³/6 (Taylor)', linewidth=2)\n",
    "axes[1].set_xlabel('z (rad)')\n",
    "axes[1].set_ylabel('Value')\n",
    "axes[1].set_title('Small Angle Approximation: sin(z) ≈ z')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXPLANATION: Small Angle Approximation\")\n",
    "print(\"=\"*60)\n",
    "print(\"\"\"\n",
    "For small angles, sin(z) ≈ z (Taylor expansion: sin(z) = z - z³/6 + ...)\n",
    "\n",
    "When training with small initial conditions, both sin(z) and z produce \n",
    "nearly identical dynamics. This creates a collinearity issue in LASSO:\n",
    "- With large angles: sin(z) is clearly distinguishable from z\n",
    "- With small angles: sin(z) ≈ z, so LASSO may assign weight to either term\n",
    "\n",
    "This is why at very small angles (z₀ < 0.1), the z term may appear with \n",
    "coefficient ≈ -1 instead of (or alongside) sin(z), as both explain the \n",
    "data equally well from the perspective of the regression objective.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e24f1c",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 2: SINDy-Autoencoder\n",
    "\n",
    "## 2.1 Cartesian Embedding\n",
    "\n",
    "Now assume we don't know the canonical coordinate $z_t$. Instead, we observe the pendulum tip in 2D Cartesian coordinates:\n",
    "$$x = [\\sin(z), -\\cos(z)]$$\n",
    "$$\\dot{x} = [\\cos(z) \\cdot \\dot{z}, \\sin(z) \\cdot \\dot{z}]$$\n",
    "$$\\ddot{x} = [-\\sin(z) \\cdot \\dot{z}^2 + \\cos(z) \\cdot \\ddot{z}, \\cos(z) \\cdot \\dot{z}^2 + \\sin(z) \\cdot \\ddot{z}]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6de2cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_cartesian(z, dz, ddz):\n",
    "    \"\"\"\n",
    "    Embed pendulum dynamics into Cartesian coordinates.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    z : ndarray\n",
    "        Angle values (can be any shape)\n",
    "    dz : ndarray\n",
    "        Angular velocity values\n",
    "    ddz : ndarray\n",
    "        Angular acceleration values\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    x : ndarray\n",
    "        Cartesian position [sin(z), -cos(z)], shape (*z.shape, 2)\n",
    "    dx : ndarray\n",
    "        Cartesian velocity, shape (*z.shape, 2)\n",
    "    ddx : ndarray\n",
    "        Cartesian acceleration, shape (*z.shape, 2)\n",
    "    \"\"\"\n",
    "    sin_z = np.sin(z)\n",
    "    cos_z = np.cos(z)\n",
    "    \n",
    "    # Position: x = [sin(z), -cos(z)]\n",
    "    x = np.stack([sin_z, -cos_z], axis=-1)\n",
    "    \n",
    "    # Velocity: dx = [cos(z)·dz, sin(z)·dz]\n",
    "    dx = np.stack([cos_z * dz, sin_z * dz], axis=-1)\n",
    "    \n",
    "    # Acceleration: ddx = [-sin(z)·dz² + cos(z)·ddz, cos(z)·dz² + sin(z)·ddz]\n",
    "    ddx = np.stack([\n",
    "        -sin_z * dz**2 + cos_z * ddz,\n",
    "        cos_z * dz**2 + sin_z * ddz\n",
    "    ], axis=-1)\n",
    "    \n",
    "    return x, dx, ddx\n",
    "\n",
    "\n",
    "# Create training data with Cartesian embedding\n",
    "train_data_cartesian = create_pendulum_data(\n",
    "    z0_min=-np.pi, z0_max=np.pi,\n",
    "    dz0_min=-2.1, dz0_max=2.1,\n",
    "    coefficients=GROUND_TRUTH_COEFFICIENTS,\n",
    "    terms=terms, T=T, dt=dt, N=N,\n",
    "    embedding='cartesian',\n",
    "    rejection=True\n",
    ")\n",
    "\n",
    "# Also compute cartesian embedding manually for verification\n",
    "x_cart, dx_cart, ddx_cart = embed_cartesian(\n",
    "    train_data['z'], train_data['dz'], train_data['ddz']\n",
    ")\n",
    "\n",
    "print(f\"Cartesian data shapes:\")\n",
    "print(f\"  x:   {x_cart.shape}\")\n",
    "print(f\"  dx:  {dx_cart.shape}\")\n",
    "print(f\"  ddx: {ddx_cart.shape}\")\n",
    "\n",
    "# Visualize Cartesian embedding\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "\n",
    "# Select one trajectory\n",
    "idx = 0\n",
    "t_plot = train_data['t'][idx]\n",
    "z_plot = train_data['z'][idx]\n",
    "x_plot = x_cart[idx]\n",
    "dx_plot = dx_cart[idx]\n",
    "ddx_plot = ddx_cart[idx]\n",
    "\n",
    "# Plot z trajectory\n",
    "axes[0, 0].plot(t_plot, z_plot, 'b-')\n",
    "axes[0, 0].set_xlabel('Time (s)')\n",
    "axes[0, 0].set_ylabel('$z$')\n",
    "axes[0, 0].set_title('Angle $z(t)$')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot x1, x2\n",
    "axes[0, 1].plot(t_plot, x_plot[:, 0], 'r-', label='$x_1 = \\\\sin(z)$')\n",
    "axes[0, 1].plot(t_plot, x_plot[:, 1], 'b-', label='$x_2 = -\\\\cos(z)$')\n",
    "axes[0, 1].set_xlabel('Time (s)')\n",
    "axes[0, 1].set_ylabel('Position')\n",
    "axes[0, 1].set_title('Cartesian Position $x(t)$')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot trajectory in x1-x2 plane\n",
    "axes[0, 2].plot(x_plot[:, 0], x_plot[:, 1], 'g-')\n",
    "axes[0, 2].scatter([x_plot[0, 0]], [x_plot[0, 1]], c='r', s=100, marker='o', label='Start')\n",
    "axes[0, 2].set_xlabel('$x_1$')\n",
    "axes[0, 2].set_ylabel('$x_2$')\n",
    "axes[0, 2].set_title('Pendulum Tip Trajectory')\n",
    "axes[0, 2].set_aspect('equal')\n",
    "axes[0, 2].legend()\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot dx1, dx2\n",
    "axes[1, 0].plot(t_plot, dx_plot[:, 0], 'r-', label='$\\dot{x}_1$')\n",
    "axes[1, 0].plot(t_plot, dx_plot[:, 1], 'b-', label='$\\dot{x}_2$')\n",
    "axes[1, 0].set_xlabel('Time (s)')\n",
    "axes[1, 0].set_ylabel('Velocity')\n",
    "axes[1, 0].set_title('Cartesian Velocity $\\dot{x}(t)$')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot ddx1, ddx2\n",
    "axes[1, 1].plot(t_plot, ddx_plot[:, 0], 'r-', label='$\\ddot{x}_1$')\n",
    "axes[1, 1].plot(t_plot, ddx_plot[:, 1], 'b-', label='$\\ddot{x}_2$')\n",
    "axes[1, 1].set_xlabel('Time (s)')\n",
    "axes[1, 1].set_ylabel('Acceleration')\n",
    "axes[1, 1].set_title('Cartesian Acceleration $\\ddot{x}(t)$')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Phase portrait in dx-space\n",
    "axes[1, 2].plot(dx_plot[:, 0], dx_plot[:, 1], 'm-')\n",
    "axes[1, 2].scatter([dx_plot[0, 0]], [dx_plot[0, 1]], c='r', s=100, marker='o', label='Start')\n",
    "axes[1, 2].set_xlabel('$\\dot{x}_1$')\n",
    "axes[1, 2].set_ylabel('$\\dot{x}_2$')\n",
    "axes[1, 2].set_title('Velocity Phase Portrait')\n",
    "axes[1, 2].legend()\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefb5754",
   "metadata": {},
   "source": [
    "## 2.2 Basic Autoencoder for Hyperparameter Search\n",
    "\n",
    "First, build a simple autoencoder to encode 2D Cartesian $x_t$ into 1D latent $z_t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84070cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicAutoencoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple autoencoder with Linear + Sigmoid layers.\n",
    "    Encodes 2D x into 1D latent z.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=2, latent_dim=1, hidden_dims=[32, 16]):\n",
    "        super(BasicAutoencoder, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        encoder_layers = []\n",
    "        prev_dim = input_dim\n",
    "        for h_dim in hidden_dims:\n",
    "            encoder_layers.append(nn.Linear(prev_dim, h_dim))\n",
    "            encoder_layers.append(nn.Sigmoid())\n",
    "            prev_dim = h_dim\n",
    "        encoder_layers.append(nn.Linear(prev_dim, latent_dim))\n",
    "        self.encoder = nn.Sequential(*encoder_layers)\n",
    "        \n",
    "        # Decoder (reverse architecture)\n",
    "        decoder_layers = []\n",
    "        prev_dim = latent_dim\n",
    "        for h_dim in reversed(hidden_dims):\n",
    "            decoder_layers.append(nn.Linear(prev_dim, h_dim))\n",
    "            decoder_layers.append(nn.Sigmoid())\n",
    "            prev_dim = h_dim\n",
    "        decoder_layers.append(nn.Linear(prev_dim, input_dim))\n",
    "        self.decoder = nn.Sequential(*decoder_layers)\n",
    "    \n",
    "    def encode(self, x):\n",
    "        return self.encoder(x)\n",
    "    \n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z = self.encode(x)\n",
    "        x_hat = self.decode(z)\n",
    "        return x_hat, z\n",
    "\n",
    "\n",
    "def train_autoencoder(model, train_loader, val_loader, n_epochs=500, lr=1e-3, verbose=True):\n",
    "    \"\"\"Train basic autoencoder.\"\"\"\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    mse_loss = nn.MSELoss()\n",
    "    \n",
    "    history = {'train_loss': [], 'val_loss': []}\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        \n",
    "        for x_batch, in train_loader:\n",
    "            x_batch = x_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            x_hat, _ = model(x_batch)\n",
    "            loss = mse_loss(x_hat, x_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "        \n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        with torch.no_grad():\n",
    "            for x_batch, in val_loader:\n",
    "                x_batch = x_batch.to(device)\n",
    "                x_hat, _ = model(x_batch)\n",
    "                loss = mse_loss(x_hat, x_batch)\n",
    "                val_losses.append(loss.item())\n",
    "        \n",
    "        history['train_loss'].append(np.mean(train_losses))\n",
    "        history['val_loss'].append(np.mean(val_losses))\n",
    "        \n",
    "        if verbose and (epoch + 1) % 100 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{n_epochs}: Train Loss = {history['train_loss'][-1]:.6f}, \"\n",
    "                  f\"Val Loss = {history['val_loss'][-1]:.6f}\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "\n",
    "# Prepare Cartesian data for autoencoder\n",
    "x_flat = x_cart.reshape(-1, 2)\n",
    "x_train, x_val = train_test_split(x_flat, test_size=0.2, random_state=42)\n",
    "\n",
    "train_ds_ae = TensorDataset(torch.tensor(x_train, dtype=torch.float32))\n",
    "val_ds_ae = TensorDataset(torch.tensor(x_val, dtype=torch.float32))\n",
    "\n",
    "train_loader_ae = DataLoader(train_ds_ae, batch_size=256, shuffle=True)\n",
    "val_loader_ae = DataLoader(val_ds_ae, batch_size=256, shuffle=False)\n",
    "\n",
    "print(f\"Autoencoder data: {len(train_ds_ae)} train, {len(val_ds_ae)} val samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f47cff4",
   "metadata": {},
   "source": [
    "### Hyperparameter Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b42f9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try different hidden layer configurations\n",
    "hidden_configs = [\n",
    "    [8],\n",
    "    [16],\n",
    "    [32],\n",
    "    [16, 8],\n",
    "    [32, 16],\n",
    "    [64, 32],\n",
    "    [32, 16, 8],\n",
    "]\n",
    "\n",
    "results_hp = []\n",
    "\n",
    "print(\"Hyperparameter search for autoencoder architecture...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for hidden_dims in hidden_configs:\n",
    "    print(f\"\\nTesting hidden_dims = {hidden_dims}\")\n",
    "    \n",
    "    model = BasicAutoencoder(input_dim=2, latent_dim=1, hidden_dims=hidden_dims).to(device)\n",
    "    history = train_autoencoder(model, train_loader_ae, val_loader_ae, \n",
    "                                n_epochs=500, lr=1e-3, verbose=False)\n",
    "    \n",
    "    final_val_loss = history['val_loss'][-1]\n",
    "    results_hp.append({\n",
    "        'hidden_dims': hidden_dims,\n",
    "        'val_loss': final_val_loss,\n",
    "        'history': history\n",
    "    })\n",
    "    print(f\"  Final val loss: {final_val_loss:.6f}\")\n",
    "\n",
    "# Find best configuration\n",
    "best_idx = np.argmin([r['val_loss'] for r in results_hp])\n",
    "best_config = results_hp[best_idx]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"Best configuration: hidden_dims = {best_config['hidden_dims']}\")\n",
    "print(f\"Best val loss: {best_config['val_loss']:.6f}\")\n",
    "\n",
    "# Visualize hyperparameter search results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bar plot of final losses\n",
    "config_names = [str(r['hidden_dims']) for r in results_hp]\n",
    "val_losses = [r['val_loss'] for r in results_hp]\n",
    "\n",
    "axes[0].bar(config_names, val_losses)\n",
    "axes[0].set_xlabel('Hidden Layer Configuration')\n",
    "axes[0].set_ylabel('Final Validation Loss')\n",
    "axes[0].set_title('Autoencoder Architecture Comparison')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Training curves for top 3\n",
    "sorted_results = sorted(results_hp, key=lambda x: x['val_loss'])[:3]\n",
    "for r in sorted_results:\n",
    "    axes[1].semilogy(r['history']['val_loss'], label=str(r['hidden_dims']))\n",
    "\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Validation Loss')\n",
    "axes[1].set_title('Training Curves (Top 3 Configurations)')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8b55cd",
   "metadata": {},
   "source": [
    "## 2.3 Propagation of Time Derivatives\n",
    "\n",
    "Implement layers that propagate time derivatives through the network using the chain rule.\n",
    "\n",
    "### Sigmoid Derivatives\n",
    "\n",
    "$$g(\\tilde{z}) = \\sigma(\\tilde{z}) = \\frac{1}{1 + e^{-\\tilde{z}}}$$\n",
    "$$g'(\\tilde{z}) = \\sigma(\\tilde{z})(1 - \\sigma(\\tilde{z}))$$\n",
    "$$g''(\\tilde{z}) = \\sigma'(\\tilde{z})(1 - 2\\sigma(\\tilde{z}))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3cf195",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SigmoidDerivatives(nn.Module):\n",
    "    \"\"\"\n",
    "    Sigmoid activation with derivative propagation.\n",
    "    \n",
    "    Forward pass computes:\n",
    "    - z = σ(x)\n",
    "    - dz = σ'(x) ⊙ dx\n",
    "    - ddz = σ''(x) ⊙ dx ⊙ dx + σ'(x) ⊙ ddx\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(SigmoidDerivatives, self).__init__()\n",
    "    \n",
    "    def forward(self, x, dx, ddx):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        x : torch.Tensor\n",
    "            Pre-activation values\n",
    "        dx : torch.Tensor\n",
    "            Time derivative of pre-activation\n",
    "        ddx : torch.Tensor\n",
    "            Second time derivative of pre-activation\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        z, dz, ddz : torch.Tensor\n",
    "            Output and its time derivatives\n",
    "        \"\"\"\n",
    "        # Sigmoid and its derivatives\n",
    "        sig = torch.sigmoid(x)\n",
    "        sig_prime = sig * (1 - sig)\n",
    "        sig_double_prime = sig_prime * (1 - 2 * sig)\n",
    "        \n",
    "        # Output\n",
    "        z = sig\n",
    "        \n",
    "        # First derivative: dz = σ'(x) ⊙ dx\n",
    "        dz = sig_prime * dx\n",
    "        \n",
    "        # Second derivative: ddz = σ''(x) ⊙ dx ⊙ dx + σ'(x) ⊙ ddx\n",
    "        ddz = sig_double_prime * dx * dx + sig_prime * ddx\n",
    "        \n",
    "        return z, dz, ddz\n",
    "\n",
    "\n",
    "class LinearDerivatives(nn.Module):\n",
    "    \"\"\"\n",
    "    Linear layer with derivative propagation.\n",
    "    \n",
    "    Forward pass computes:\n",
    "    - z = xW + b\n",
    "    - dz = dx @ W\n",
    "    - ddz = ddx @ W\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features, bias=False):\n",
    "        super(LinearDerivatives, self).__init__()\n",
    "        self.linear = nn.Linear(in_features, out_features, bias=bias)\n",
    "        \n",
    "        # Xavier uniform initialization\n",
    "        nn.init.xavier_uniform_(self.linear.weight)\n",
    "        if bias and self.linear.bias is not None:\n",
    "            nn.init.zeros_(self.linear.bias)\n",
    "    \n",
    "    def forward(self, x, dx, ddx):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        x : torch.Tensor\n",
    "            Input values\n",
    "        dx : torch.Tensor\n",
    "            Time derivative of input\n",
    "        ddx : torch.Tensor\n",
    "            Second time derivative of input\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        z, dz, ddz : torch.Tensor\n",
    "            Output and its time derivatives\n",
    "        \"\"\"\n",
    "        # Linear transformation\n",
    "        z = self.linear(x)\n",
    "        \n",
    "        # Derivatives (just multiply by weight, no bias)\n",
    "        W = self.linear.weight.t()  # Shape: (in_features, out_features)\n",
    "        dz = dx @ W\n",
    "        ddz = ddx @ W\n",
    "        \n",
    "        return z, dz, ddz\n",
    "    \n",
    "    @property\n",
    "    def weight(self):\n",
    "        return self.linear.weight\n",
    "\n",
    "\n",
    "# Test derivative propagation layers\n",
    "print(\"Testing SigmoidDerivatives...\")\n",
    "sig_layer = SigmoidDerivatives()\n",
    "x_test = torch.randn(10, 5)\n",
    "dx_test = torch.randn(10, 5)\n",
    "ddx_test = torch.randn(10, 5)\n",
    "z, dz, ddz = sig_layer(x_test, dx_test, ddx_test)\n",
    "print(f\"  Input shape: {x_test.shape}, Output shape: {z.shape}\")\n",
    "\n",
    "print(\"\\nTesting LinearDerivatives...\")\n",
    "lin_layer = LinearDerivatives(5, 3)\n",
    "z, dz, ddz = lin_layer(x_test, dx_test, ddx_test)\n",
    "print(f\"  Input shape: {x_test.shape}, Output shape: {z.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac2fc37",
   "metadata": {},
   "source": [
    "### Verify Derivative Propagation\n",
    "\n",
    "Compare propagated derivatives with finite difference approximations:\n",
    "$$\\dot{z}_t \\approx \\frac{z_{t+1} - z_{t-1}}{2\\Delta t}$$\n",
    "$$\\ddot{z}_t \\approx \\frac{z_{t-1} - 2z_t + z_{t+1}}{\\Delta t^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be50c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_derivative_propagation(model, x, dx, ddx, dt):\n",
    "    \"\"\"\n",
    "    Verify derivative propagation by comparing with finite differences.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : nn.Module\n",
    "        Model with forward(x, dx, ddx) method\n",
    "    x : ndarray\n",
    "        Input sequence of shape (T, D)\n",
    "    dx, ddx : ndarray\n",
    "        True derivatives\n",
    "    dt : float\n",
    "        Time step\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    errors : dict\n",
    "        Dictionary with error metrics\n",
    "    \"\"\"\n",
    "    T = x.shape[0]\n",
    "    \n",
    "    # Convert to tensors\n",
    "    x_t = torch.tensor(x, dtype=torch.float32).to(device)\n",
    "    dx_t = torch.tensor(dx, dtype=torch.float32).to(device)\n",
    "    ddx_t = torch.tensor(ddx, dtype=torch.float32).to(device)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Get propagated derivatives\n",
    "        z, dz_prop, ddz_prop = model(x_t, dx_t, ddx_t)\n",
    "        \n",
    "        z = z.cpu().numpy()\n",
    "        dz_prop = dz_prop.cpu().numpy()\n",
    "        ddz_prop = ddz_prop.cpu().numpy()\n",
    "    \n",
    "    # Compute finite difference derivatives\n",
    "    # dz_fd[t] = (z[t+1] - z[t-1]) / (2*dt)\n",
    "    # ddz_fd[t] = (z[t-1] - 2*z[t] + z[t+1]) / dt²\n",
    "    \n",
    "    dz_fd = np.zeros_like(z)\n",
    "    ddz_fd = np.zeros_like(z)\n",
    "    \n",
    "    for t in range(1, T-1):\n",
    "        dz_fd[t] = (z[t+1] - z[t-1]) / (2 * dt)\n",
    "        ddz_fd[t] = (z[t-1] - 2*z[t] + z[t+1]) / (dt**2)\n",
    "    \n",
    "    # Compute errors (excluding boundary points)\n",
    "    dz_error = np.mean(np.abs(dz_prop[1:-1] - dz_fd[1:-1]))\n",
    "    ddz_error = np.mean(np.abs(ddz_prop[1:-1] - ddz_fd[1:-1]))\n",
    "    \n",
    "    return {\n",
    "        'dz_prop': dz_prop,\n",
    "        'dz_fd': dz_fd,\n",
    "        'ddz_prop': ddz_prop,\n",
    "        'ddz_fd': ddz_fd,\n",
    "        'dz_error': dz_error,\n",
    "        'ddz_error': ddz_error,\n",
    "        'z': z\n",
    "    }\n",
    "\n",
    "\n",
    "# Build a simple encoder for verification\n",
    "class TestEncoder(nn.Module):\n",
    "    def __init__(self, input_dim=2, hidden_dim=16, output_dim=1):\n",
    "        super(TestEncoder, self).__init__()\n",
    "        self.lin1 = LinearDerivatives(input_dim, hidden_dim, bias=False)\n",
    "        self.sig1 = SigmoidDerivatives()\n",
    "        self.lin2 = LinearDerivatives(hidden_dim, output_dim, bias=False)\n",
    "    \n",
    "    def forward(self, x, dx, ddx):\n",
    "        z, dz, ddz = self.lin1(x, dx, ddx)\n",
    "        z, dz, ddz = self.sig1(z, dz, ddz)\n",
    "        z, dz, ddz = self.lin2(z, dz, ddz)\n",
    "        return z, dz, ddz\n",
    "\n",
    "\n",
    "# Test with actual data\n",
    "test_encoder = TestEncoder(input_dim=2, hidden_dim=16, output_dim=1).to(device)\n",
    "\n",
    "# Use one trajectory\n",
    "idx = 0\n",
    "x_seq = x_cart[idx]      # (T, 2)\n",
    "dx_seq = dx_cart[idx]\n",
    "ddx_seq = ddx_cart[idx]\n",
    "\n",
    "verification = verify_derivative_propagation(test_encoder, x_seq, dx_seq, ddx_seq, dt)\n",
    "\n",
    "print(\"Derivative Propagation Verification:\")\n",
    "print(f\"  Mean |dz_prop - dz_fd|:   {verification['dz_error']:.6f}\")\n",
    "print(f\"  Mean |ddz_prop - ddz_fd|: {verification['ddz_error']:.6f}\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "t_plot = np.arange(len(verification['z'])) * dt\n",
    "\n",
    "# Plot z\n",
    "axes[0].plot(t_plot, verification['z'], 'b-')\n",
    "axes[0].set_xlabel('Time (s)')\n",
    "axes[0].set_ylabel('z')\n",
    "axes[0].set_title('Encoded z(t)')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot first derivative comparison\n",
    "axes[1].plot(t_plot, verification['dz_prop'], 'b-', label='Propagated')\n",
    "axes[1].plot(t_plot, verification['dz_fd'], 'r--', label='Finite Diff')\n",
    "axes[1].set_xlabel('Time (s)')\n",
    "axes[1].set_ylabel('dz/dt')\n",
    "axes[1].set_title('First Derivative Comparison')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot second derivative comparison\n",
    "axes[2].plot(t_plot, verification['ddz_prop'], 'b-', label='Propagated')\n",
    "axes[2].plot(t_plot, verification['ddz_fd'], 'r--', label='Finite Diff')\n",
    "axes[2].set_xlabel('Time (s)')\n",
    "axes[2].set_ylabel('d²z/dt²')\n",
    "axes[2].set_title('Second Derivative Comparison')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nNote: Small differences are expected due to finite difference approximation error.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d3af28",
   "metadata": {},
   "source": [
    "## 2.4 SINDy-Autoencoder Implementation\n",
    "\n",
    "Implement `SINDyAutoencoder` containing:\n",
    "- Internal SINDy instance\n",
    "- Encoder/decoder with derivative propagation\n",
    "- Xavier uniform initialization, no biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd174db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SINDyAutoencoder(nn.Module):\n",
    "    \"\"\"\n",
    "    SINDy-Autoencoder combining an autoencoder with SINDy.\n",
    "    \n",
    "    The encoder learns to map x -> z (latent coordinate).\n",
    "    SINDy learns the dynamics ddz = Θ(z, dz) · Ξ.\n",
    "    The decoder reconstructs x_hat from z.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=2, latent_dim=1, encoder_hidden=[32, 16], \n",
    "                 decoder_hidden=[16, 32], n_sindy_terms=10):\n",
    "        super(SINDyAutoencoder, self).__init__()\n",
    "        \n",
    "        # Build encoder with derivative propagation\n",
    "        self.encoder_layers = nn.ModuleList()\n",
    "        prev_dim = input_dim\n",
    "        for h_dim in encoder_hidden:\n",
    "            self.encoder_layers.append(LinearDerivatives(prev_dim, h_dim, bias=False))\n",
    "            self.encoder_layers.append(SigmoidDerivatives())\n",
    "            prev_dim = h_dim\n",
    "        self.encoder_layers.append(LinearDerivatives(prev_dim, latent_dim, bias=False))\n",
    "        \n",
    "        # Build decoder with derivative propagation\n",
    "        self.decoder_layers = nn.ModuleList()\n",
    "        prev_dim = latent_dim\n",
    "        for h_dim in decoder_hidden:\n",
    "            self.decoder_layers.append(LinearDerivatives(prev_dim, h_dim, bias=False))\n",
    "            self.decoder_layers.append(SigmoidDerivatives())\n",
    "            prev_dim = h_dim\n",
    "        self.decoder_layers.append(LinearDerivatives(prev_dim, input_dim, bias=False))\n",
    "        \n",
    "        # SINDy module\n",
    "        self.sindy = SINDy(n_terms=n_sindy_terms)\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "    \n",
    "    def encode(self, x, dx, ddx):\n",
    "        \"\"\"Encode x to z with derivative propagation.\"\"\"\n",
    "        z, dz, ddz = x, dx, ddx\n",
    "        for layer in self.encoder_layers:\n",
    "            z, dz, ddz = layer(z, dz, ddz)\n",
    "        return z, dz, ddz\n",
    "    \n",
    "    def decode(self, z, dz=None, ddz=None):\n",
    "        \"\"\"\n",
    "        Decode z to x_hat.\n",
    "        If dz and ddz are provided, also propagate derivatives.\n",
    "        \"\"\"\n",
    "        if dz is None:\n",
    "            # Simple decode without derivatives (for reconstruction only)\n",
    "            x = z\n",
    "            for layer in self.decoder_layers:\n",
    "                if isinstance(layer, LinearDerivatives):\n",
    "                    x = layer.linear(x)\n",
    "                elif isinstance(layer, SigmoidDerivatives):\n",
    "                    x = torch.sigmoid(x)\n",
    "            return x\n",
    "        else:\n",
    "            # Decode with derivative propagation\n",
    "            x, dx, ddx = z, dz, ddz\n",
    "            for layer in self.decoder_layers:\n",
    "                x, dx, ddx = layer(x, dx, ddx)\n",
    "            return x, dx, ddx\n",
    "    \n",
    "    def forward(self, x, dx, ddx):\n",
    "        \"\"\"\n",
    "        Forward pass computing:\n",
    "        - x_hat: reconstructed x\n",
    "        - ddz_hat: predicted ddz from SINDy\n",
    "        - ddx_hat: ddz decoded back to x-space\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        x, dx, ddx : torch.Tensor\n",
    "            Input and its time derivatives\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        x_hat : torch.Tensor\n",
    "            Reconstructed x\n",
    "        ddz_hat : torch.Tensor\n",
    "            SINDy prediction of ddz\n",
    "        ddx_hat : torch.Tensor\n",
    "            SINDy prediction decoded to x-space\n",
    "        z : torch.Tensor\n",
    "            Encoded latent variable\n",
    "        dz : torch.Tensor\n",
    "            Encoded latent velocity\n",
    "        ddz : torch.Tensor\n",
    "            Encoded latent acceleration (for loss computation)\n",
    "        \"\"\"\n",
    "        # Encode\n",
    "        z, dz, ddz_enc = self.encode(x, dx, ddx)\n",
    "        \n",
    "        # SINDy: predict ddz from z and dz\n",
    "        ddz_hat = self.sindy(z.squeeze(-1), dz.squeeze(-1)).unsqueeze(-1)\n",
    "        \n",
    "        # Decode reconstruction (z only)\n",
    "        x_hat = self.decode(z)\n",
    "        \n",
    "        # Decode SINDy prediction to x-space\n",
    "        # We need to propagate the predicted ddz through the decoder\n",
    "        # Using zero for dz since we're only interested in the ddx\n",
    "        # Actually, we use the encoded z and dz, and replace ddz with ddz_hat\n",
    "        _, _, ddx_hat = self.decode(z, dz, ddz_hat)\n",
    "        \n",
    "        return x_hat, ddz_hat, ddx_hat, z, dz, ddz_enc\n",
    "    \n",
    "    def get_sindy_coefficients(self):\n",
    "        return self.sindy.get_coefficients()\n",
    "    \n",
    "    def get_sindy_mask(self):\n",
    "        return self.sindy.get_mask()\n",
    "    \n",
    "    def sindy_l1_regularization(self):\n",
    "        return self.sindy.l1_regularization()\n",
    "\n",
    "\n",
    "# Test SINDy-Autoencoder\n",
    "sindy_ae = SINDyAutoencoder(\n",
    "    input_dim=2, latent_dim=1, \n",
    "    encoder_hidden=[32, 16], decoder_hidden=[16, 32],\n",
    "    n_sindy_terms=10\n",
    ").to(device)\n",
    "\n",
    "# Test forward pass\n",
    "x_test = torch.randn(100, 2).to(device)\n",
    "dx_test = torch.randn(100, 2).to(device)\n",
    "ddx_test = torch.randn(100, 2).to(device)\n",
    "\n",
    "x_hat, ddz_hat, ddx_hat, z, dz, ddz = sindy_ae(x_test, dx_test, ddx_test)\n",
    "\n",
    "print(\"SINDy-Autoencoder Test:\")\n",
    "print(f\"  Input x shape: {x_test.shape}\")\n",
    "print(f\"  Reconstructed x_hat shape: {x_hat.shape}\")\n",
    "print(f\"  Latent z shape: {z.shape}\")\n",
    "print(f\"  SINDy ddz_hat shape: {ddz_hat.shape}\")\n",
    "print(f\"  Decoded ddx_hat shape: {ddx_hat.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d228713b",
   "metadata": {},
   "source": [
    "### Training Function for SINDy-Autoencoder\n",
    "\n",
    "The loss function combines:\n",
    "$$\\mathcal{L} = |x - \\hat{x}|_2^2 + \\lambda_{\\ddot{z}}|\\ddot{z} - \\hat{\\ddot{z}}|_2^2 + \\lambda_{\\ddot{x}}|\\ddot{x} - \\hat{\\ddot{x}}|_2^2 + \\lambda_1|\\Xi|_1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedcf725",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sindy_autoencoder(model, train_loader, val_loader,\n",
    "                           n_epochs=5000, lr=1e-3,\n",
    "                           lambda_ddz=5e-5, lambda_ddx=5e-4, lambda_l1=1e-5,\n",
    "                           thresholding=None, threshold_a=0.1, threshold_b=0.002,\n",
    "                           threshold_interval=500, patience=1000,\n",
    "                           refinement_epoch=None,\n",
    "                           verbose=True, log_interval=100):\n",
    "    \"\"\"\n",
    "    Train SINDy-Autoencoder.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : SINDyAutoencoder\n",
    "        Model to train\n",
    "    train_loader, val_loader : DataLoader\n",
    "        Data loaders with (x, dx, ddx) tuples\n",
    "    n_epochs : int\n",
    "        Total training epochs\n",
    "    lr : float\n",
    "        Learning rate\n",
    "    lambda_ddz, lambda_ddx : float\n",
    "        Loss weights for latent and decoded acceleration\n",
    "    lambda_l1 : float\n",
    "        L1 regularization weight\n",
    "    thresholding : str or None\n",
    "        'sequential', 'patient', or None\n",
    "    threshold_a, threshold_b : float\n",
    "        Thresholding parameters\n",
    "    threshold_interval : int\n",
    "        For sequential thresholding\n",
    "    patience : int\n",
    "        For patient thresholding\n",
    "    refinement_epoch : int or None\n",
    "        Epoch to start refinement (λ₁ = 0)\n",
    "    verbose : bool\n",
    "        Print progress\n",
    "    log_interval : int\n",
    "        Print every log_interval epochs\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    history : dict\n",
    "        Training history\n",
    "    \"\"\"\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    mse_loss = nn.MSELoss()\n",
    "    \n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'val_loss': [],\n",
    "        'train_loss_x': [],\n",
    "        'train_loss_ddz': [],\n",
    "        'train_loss_ddx': [],\n",
    "        'val_loss_x': [],\n",
    "        'val_loss_ddz': [],\n",
    "        'val_loss_ddx': [],\n",
    "        'coefficients': [],\n",
    "        'mask': []\n",
    "    }\n",
    "    \n",
    "    # For PTAT\n",
    "    if thresholding == 'patient':\n",
    "        xi_prev = torch.zeros_like(model.sindy.coefficients)\n",
    "        E_a = torch.zeros(model.sindy.n_terms, device=device)\n",
    "        E_b = torch.zeros(model.sindy.n_terms, device=device)\n",
    "    \n",
    "    current_lambda_l1 = lambda_l1\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        # Check if refinement should start\n",
    "        if refinement_epoch is not None and epoch >= refinement_epoch:\n",
    "            current_lambda_l1 = 0.0\n",
    "        \n",
    "        # Training\n",
    "        model.train()\n",
    "        train_losses = {'total': [], 'x': [], 'ddz': [], 'ddx': []}\n",
    "        \n",
    "        for x_batch, dx_batch, ddx_batch in train_loader:\n",
    "            x_batch = x_batch.to(device)\n",
    "            dx_batch = dx_batch.to(device)\n",
    "            ddx_batch = ddx_batch.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            x_hat, ddz_hat, ddx_hat, z, dz, ddz = model(x_batch, dx_batch, ddx_batch)\n",
    "            \n",
    "            # Compute losses\n",
    "            loss_x = mse_loss(x_hat, x_batch)\n",
    "            loss_ddz = mse_loss(ddz_hat.squeeze(), ddz.squeeze())\n",
    "            loss_ddx = mse_loss(ddx_hat, ddx_batch)\n",
    "            \n",
    "            # Total loss\n",
    "            loss = loss_x + lambda_ddz * loss_ddz + lambda_ddx * loss_ddx\n",
    "            \n",
    "            # Add L1 regularization\n",
    "            if current_lambda_l1 > 0:\n",
    "                loss += current_lambda_l1 * model.sindy_l1_regularization()\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_losses['total'].append(loss.item())\n",
    "            train_losses['x'].append(loss_x.item())\n",
    "            train_losses['ddz'].append(loss_ddz.item())\n",
    "            train_losses['ddx'].append(loss_ddx.item())\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_losses = {'total': [], 'x': [], 'ddz': [], 'ddx': []}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for x_batch, dx_batch, ddx_batch in val_loader:\n",
    "                x_batch = x_batch.to(device)\n",
    "                dx_batch = dx_batch.to(device)\n",
    "                ddx_batch = ddx_batch.to(device)\n",
    "                \n",
    "                x_hat, ddz_hat, ddx_hat, z, dz, ddz = model(x_batch, dx_batch, ddx_batch)\n",
    "                \n",
    "                loss_x = mse_loss(x_hat, x_batch)\n",
    "                loss_ddz = mse_loss(ddz_hat.squeeze(), ddz.squeeze())\n",
    "                loss_ddx = mse_loss(ddx_hat, ddx_batch)\n",
    "                \n",
    "                loss = loss_x + lambda_ddz * loss_ddz + lambda_ddx * loss_ddx\n",
    "                \n",
    "                val_losses['total'].append(loss.item())\n",
    "                val_losses['x'].append(loss_x.item())\n",
    "                val_losses['ddz'].append(loss_ddz.item())\n",
    "                val_losses['ddx'].append(loss_ddx.item())\n",
    "        \n",
    "        # Record history\n",
    "        history['train_loss'].append(np.mean(train_losses['total']))\n",
    "        history['val_loss'].append(np.mean(val_losses['total']))\n",
    "        history['train_loss_x'].append(np.mean(train_losses['x']))\n",
    "        history['train_loss_ddz'].append(np.mean(train_losses['ddz']))\n",
    "        history['train_loss_ddx'].append(np.mean(train_losses['ddx']))\n",
    "        history['val_loss_x'].append(np.mean(val_losses['x']))\n",
    "        history['val_loss_ddz'].append(np.mean(val_losses['ddz']))\n",
    "        history['val_loss_ddx'].append(np.mean(val_losses['ddx']))\n",
    "        history['coefficients'].append(model.get_sindy_coefficients().copy())\n",
    "        history['mask'].append(model.get_sindy_mask().copy())\n",
    "        \n",
    "        # Apply thresholding\n",
    "        if thresholding == 'sequential':\n",
    "            if (epoch + 1) % threshold_interval == 0:\n",
    "                with torch.no_grad():\n",
    "                    coeffs = model.sindy.coefficients.abs()\n",
    "                    mask = model.sindy.mask.clone()\n",
    "                    \n",
    "                    small_coeffs = coeffs < threshold_a\n",
    "                    model.sindy.coefficients.data[small_coeffs] = 0\n",
    "                    mask[small_coeffs] = False\n",
    "                    model.sindy.mask.copy_(mask)\n",
    "                    \n",
    "                    if verbose:\n",
    "                        n_active = mask.sum().item()\n",
    "                        print(f\"Epoch {epoch+1}: Thresholded, {n_active} active terms\")\n",
    "        \n",
    "        elif thresholding == 'patient':\n",
    "            with torch.no_grad():\n",
    "                coeffs = model.sindy.coefficients.clone()\n",
    "                \n",
    "                over_a = coeffs.abs() > threshold_a\n",
    "                E_a[over_a] = epoch + 1\n",
    "                \n",
    "                delta = (coeffs - xi_prev).abs() > threshold_b\n",
    "                E_b[delta] = epoch + 1\n",
    "                \n",
    "                within_patience_a = (epoch + 1 - E_a) < patience\n",
    "                within_patience_b = (epoch + 1 - E_b) < patience\n",
    "                temp_mask = within_patience_a | within_patience_b\n",
    "                \n",
    "                new_mask = model.sindy.mask & temp_mask\n",
    "                model.sindy.mask.copy_(new_mask)\n",
    "                \n",
    "                model.sindy.coefficients.data[~model.sindy.mask] = 0\n",
    "                \n",
    "                xi_prev = coeffs.clone()\n",
    "        \n",
    "        # Print progress\n",
    "        if verbose and (epoch + 1) % log_interval == 0:\n",
    "            n_active = model.sindy.mask.sum().item()\n",
    "            ref_status = \" [REFINEMENT]\" if (refinement_epoch and epoch >= refinement_epoch) else \"\"\n",
    "            print(f\"Epoch {epoch+1}/{n_epochs}: Loss = {history['train_loss'][-1]:.6f}, \"\n",
    "                  f\"Active = {n_active}{ref_status}\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "\n",
    "# Prepare data loaders for SINDy-Autoencoder\n",
    "x_ae = x_cart.reshape(-1, 2)\n",
    "dx_ae = dx_cart.reshape(-1, 2)\n",
    "ddx_ae = ddx_cart.reshape(-1, 2)\n",
    "\n",
    "# Split\n",
    "x_tr, x_vl, dx_tr, dx_vl, ddx_tr, ddx_vl = train_test_split(\n",
    "    x_ae, dx_ae, ddx_ae, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "train_ds_sae = TensorDataset(\n",
    "    torch.tensor(x_tr, dtype=torch.float32),\n",
    "    torch.tensor(dx_tr, dtype=torch.float32),\n",
    "    torch.tensor(ddx_tr, dtype=torch.float32)\n",
    ")\n",
    "val_ds_sae = TensorDataset(\n",
    "    torch.tensor(x_vl, dtype=torch.float32),\n",
    "    torch.tensor(dx_vl, dtype=torch.float32),\n",
    "    torch.tensor(ddx_vl, dtype=torch.float32)\n",
    ")\n",
    "\n",
    "train_loader_sae = DataLoader(train_ds_sae, batch_size=256, shuffle=True)\n",
    "val_loader_sae = DataLoader(val_ds_sae, batch_size=256, shuffle=False)\n",
    "\n",
    "print(f\"SINDy-Autoencoder data: {len(train_ds_sae)} train, {len(val_ds_sae)} val\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9173ade",
   "metadata": {},
   "source": [
    "## 2.6 Training SINDy-Autoencoder\n",
    "\n",
    "Train with:\n",
    "- 5000 epochs + 1000 epochs refinement\n",
    "- Sequential Thresholding: $a=0.1$, $S=500$ epochs\n",
    "- Loss weights: $\\lambda_{\\ddot{z}}=5 \\times 10^{-5}$, $\\lambda_{\\ddot{x}}=5 \\times 10^{-4}$, $\\lambda_{L1}=10^{-5}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddb658d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train SINDy-Autoencoder with Sequential Thresholding\n",
    "print(\"Training SINDy-Autoencoder with Sequential Thresholding...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "sindy_ae_st = SINDyAutoencoder(\n",
    "    input_dim=2, latent_dim=1,\n",
    "    encoder_hidden=[32, 16], decoder_hidden=[16, 32],\n",
    "    n_sindy_terms=10\n",
    ").to(device)\n",
    "\n",
    "history_sae_st = train_sindy_autoencoder(\n",
    "    sindy_ae_st, train_loader_sae, val_loader_sae,\n",
    "    n_epochs=6000,  # 5000 + 1000 refinement\n",
    "    lr=1e-3,\n",
    "    lambda_ddz=5e-5, lambda_ddx=5e-4, lambda_l1=1e-5,\n",
    "    thresholding='sequential',\n",
    "    threshold_a=0.1, threshold_interval=500,\n",
    "    refinement_epoch=5000,\n",
    "    verbose=True, log_interval=500\n",
    ")\n",
    "\n",
    "# Get final results\n",
    "st_ae_coeffs = sindy_ae_st.get_sindy_coefficients()\n",
    "st_ae_mask = sindy_ae_st.get_sindy_mask()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SINDy-Autoencoder (Sequential Thresholding) Results:\")\n",
    "print(\"-\"*50)\n",
    "for name, coef, mask in zip(term_names, st_ae_coeffs, st_ae_mask):\n",
    "    status = \"Active\" if mask else \"Inactive\"\n",
    "    print(f\"  {name:15s}: {coef:10.6f} [{status}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa322292",
   "metadata": {},
   "source": [
    "### Visualize Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89841a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sindy_ae_history(history, title, term_names, refinement_epoch=None):\n",
    "    \"\"\"Plot SINDy-Autoencoder training history.\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    epochs = np.arange(len(history['train_loss']))\n",
    "    \n",
    "    # Total loss\n",
    "    axes[0, 0].semilogy(epochs, history['train_loss'], label='Train', alpha=0.8)\n",
    "    axes[0, 0].semilogy(epochs, history['val_loss'], label='Val', alpha=0.8)\n",
    "    if refinement_epoch:\n",
    "        axes[0, 0].axvline(x=refinement_epoch, color='r', linestyle='--', label='Refinement')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Total Loss')\n",
    "    axes[0, 0].set_title(f'{title} - Total Loss')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Component losses\n",
    "    axes[0, 1].semilogy(epochs, history['train_loss_x'], label='$L_x$', alpha=0.8)\n",
    "    axes[0, 1].semilogy(epochs, history['train_loss_ddz'], label='$L_{\\\\ddot{z}}$', alpha=0.8)\n",
    "    axes[0, 1].semilogy(epochs, history['train_loss_ddx'], label='$L_{\\\\ddot{x}}$', alpha=0.8)\n",
    "    if refinement_epoch:\n",
    "        axes[0, 1].axvline(x=refinement_epoch, color='r', linestyle='--', label='Refinement')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Loss')\n",
    "    axes[0, 1].set_title(f'{title} - Loss Components')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Coefficient evolution\n",
    "    coeffs_history = np.array(history['coefficients'])\n",
    "    for i, name in enumerate(term_names):\n",
    "        axes[1, 0].plot(epochs, coeffs_history[:, i], label=name, alpha=0.8)\n",
    "    if refinement_epoch:\n",
    "        axes[1, 0].axvline(x=refinement_epoch, color='r', linestyle='--', label='Refinement')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('Coefficient Value')\n",
    "    axes[1, 0].set_title(f'{title} - Coefficient Evolution')\n",
    "    axes[1, 0].legend(fontsize=8, loc='upper right')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Final coefficients\n",
    "    final_coeffs = coeffs_history[-1]\n",
    "    colors = ['green' if np.abs(c) > 0.05 else 'gray' for c in final_coeffs]\n",
    "    bars = axes[1, 1].bar(term_names, final_coeffs, color=colors)\n",
    "    axes[1, 1].axhline(y=0, color='k', linestyle='-', linewidth=0.5)\n",
    "    axes[1, 1].set_xlabel('Term')\n",
    "    axes[1, 1].set_ylabel('Coefficient')\n",
    "    axes[1, 1].set_title(f'{title} - Final Coefficients')\n",
    "    axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Plot training history\n",
    "plot_sindy_ae_history(history_sae_st, \"Sequential Thresholding\", term_names, refinement_epoch=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72f41cb",
   "metadata": {},
   "source": [
    "### Train with Patient Trend-Aware Thresholding (PTAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addde3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train SINDy-Autoencoder with PTAT\n",
    "print(\"Training SINDy-Autoencoder with Patient Trend-Aware Thresholding...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "sindy_ae_ptat = SINDyAutoencoder(\n",
    "    input_dim=2, latent_dim=1,\n",
    "    encoder_hidden=[32, 16], decoder_hidden=[16, 32],\n",
    "    n_sindy_terms=10\n",
    ").to(device)\n",
    "\n",
    "history_sae_ptat = train_sindy_autoencoder(\n",
    "    sindy_ae_ptat, train_loader_sae, val_loader_sae,\n",
    "    n_epochs=6000,\n",
    "    lr=1e-3,\n",
    "    lambda_ddz=5e-5, lambda_ddx=5e-4, lambda_l1=1e-5,\n",
    "    thresholding='patient',\n",
    "    threshold_a=0.1, threshold_b=0.002, patience=1000,\n",
    "    refinement_epoch=5000,\n",
    "    verbose=True, log_interval=500\n",
    ")\n",
    "\n",
    "# Get final results\n",
    "ptat_ae_coeffs = sindy_ae_ptat.get_sindy_coefficients()\n",
    "ptat_ae_mask = sindy_ae_ptat.get_sindy_mask()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SINDy-Autoencoder (PTAT) Results:\")\n",
    "print(\"-\"*50)\n",
    "for name, coef, mask in zip(term_names, ptat_ae_coeffs, ptat_ae_mask):\n",
    "    status = \"Active\" if mask else \"Inactive\"\n",
    "    print(f\"  {name:15s}: {coef:10.6f} [{status}]\")\n",
    "\n",
    "# Plot\n",
    "plot_sindy_ae_history(history_sae_ptat, \"PTAT\", term_names, refinement_epoch=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6024e6e0",
   "metadata": {},
   "source": [
    "### Train Multiple Models for Statistical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38763edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train 10 models with ST and PTAT for statistical analysis\n",
    "# Note: This cell takes longer to run\n",
    "\n",
    "n_models = 10\n",
    "all_results_st = []\n",
    "all_results_ptat = []\n",
    "\n",
    "print(f\"Training {n_models} models with each thresholding method...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i in range(n_models):\n",
    "    print(f\"\\n--- Model {i+1}/{n_models} ---\")\n",
    "    \n",
    "    # Train ST model\n",
    "    model_st = SINDyAutoencoder(\n",
    "        input_dim=2, latent_dim=1,\n",
    "        encoder_hidden=[32, 16], decoder_hidden=[16, 32],\n",
    "        n_sindy_terms=10\n",
    "    ).to(device)\n",
    "    \n",
    "    history_st = train_sindy_autoencoder(\n",
    "        model_st, train_loader_sae, val_loader_sae,\n",
    "        n_epochs=6000, lr=1e-3,\n",
    "        lambda_ddz=5e-5, lambda_ddx=5e-4, lambda_l1=1e-5,\n",
    "        thresholding='sequential',\n",
    "        threshold_a=0.1, threshold_interval=500,\n",
    "        refinement_epoch=5000,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    all_results_st.append({\n",
    "        'coefficients': model_st.get_sindy_coefficients(),\n",
    "        'mask': model_st.get_sindy_mask(),\n",
    "        'model': model_st,\n",
    "        'history': history_st\n",
    "    })\n",
    "    print(f\"  ST: {model_st.get_sindy_mask().sum()} active terms\")\n",
    "    \n",
    "    # Train PTAT model\n",
    "    model_ptat = SINDyAutoencoder(\n",
    "        input_dim=2, latent_dim=1,\n",
    "        encoder_hidden=[32, 16], decoder_hidden=[16, 32],\n",
    "        n_sindy_terms=10\n",
    "    ).to(device)\n",
    "    \n",
    "    history_ptat = train_sindy_autoencoder(\n",
    "        model_ptat, train_loader_sae, val_loader_sae,\n",
    "        n_epochs=6000, lr=1e-3,\n",
    "        lambda_ddz=5e-5, lambda_ddx=5e-4, lambda_l1=1e-5,\n",
    "        thresholding='patient',\n",
    "        threshold_a=0.1, threshold_b=0.002, patience=1000,\n",
    "        refinement_epoch=5000,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    all_results_ptat.append({\n",
    "        'coefficients': model_ptat.get_sindy_coefficients(),\n",
    "        'mask': model_ptat.get_sindy_mask(),\n",
    "        'model': model_ptat,\n",
    "        'history': history_ptat\n",
    "    })\n",
    "    print(f\"  PTAT: {model_ptat.get_sindy_mask().sum()} active terms\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c65324",
   "metadata": {},
   "source": [
    "## 2.7 Evaluation & Visualization\n",
    "\n",
    "### Compute FVU Metrics\n",
    "\n",
    "Fraction of Variance Unexplained:\n",
    "$$\\text{FVU}_y = \\frac{\\sum_i (y_i - \\hat{y}_i)^2}{\\sum_i (y_i - \\bar{y})^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c3723f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_fvu(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute Fraction of Variance Unexplained.\n",
    "    \n",
    "    FVU = Σ(y - ŷ)² / Σ(y - ȳ)²\n",
    "    \"\"\"\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    \n",
    "    ss_res = np.sum((y_true - y_pred)**2)\n",
    "    ss_tot = np.sum((y_true - np.mean(y_true))**2)\n",
    "    \n",
    "    return ss_res / ss_tot if ss_tot > 0 else 0.0\n",
    "\n",
    "\n",
    "def evaluate_sindy_autoencoder(model, x, dx, ddx):\n",
    "    \"\"\"\n",
    "    Evaluate SINDy-Autoencoder and compute FVU metrics.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x_t = torch.tensor(x, dtype=torch.float32).to(device)\n",
    "        dx_t = torch.tensor(dx, dtype=torch.float32).to(device)\n",
    "        ddx_t = torch.tensor(ddx, dtype=torch.float32).to(device)\n",
    "        \n",
    "        x_hat, ddz_hat, ddx_hat, z, dz, ddz = model(x_t, dx_t, ddx_t)\n",
    "        \n",
    "        x_hat = x_hat.cpu().numpy()\n",
    "        ddz_hat = ddz_hat.cpu().numpy()\n",
    "        ddx_hat = ddx_hat.cpu().numpy()\n",
    "        ddz = ddz.cpu().numpy()\n",
    "    \n",
    "    # Compute FVUs\n",
    "    fvu_x = compute_fvu(x.flatten(), x_hat.flatten())\n",
    "    fvu_ddz = compute_fvu(ddz.flatten(), ddz_hat.flatten())\n",
    "    fvu_ddx = compute_fvu(ddx.flatten(), ddx_hat.flatten())\n",
    "    \n",
    "    return {\n",
    "        'fvu_x': fvu_x,\n",
    "        'fvu_ddz': fvu_ddz,\n",
    "        'fvu_ddx': fvu_ddx,\n",
    "        'x_hat': x_hat,\n",
    "        'ddz_hat': ddz_hat,\n",
    "        'ddx_hat': ddx_hat\n",
    "    }\n",
    "\n",
    "\n",
    "# Evaluate all models\n",
    "print(\"Evaluating models...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "x_val = x_vl  # Validation data\n",
    "dx_val = dx_vl\n",
    "ddx_val = ddx_vl\n",
    "\n",
    "fvu_results_st = []\n",
    "fvu_results_ptat = []\n",
    "\n",
    "for i, (r_st, r_ptat) in enumerate(zip(all_results_st, all_results_ptat)):\n",
    "    eval_st = evaluate_sindy_autoencoder(r_st['model'], x_val, dx_val, ddx_val)\n",
    "    eval_ptat = evaluate_sindy_autoencoder(r_ptat['model'], x_val, dx_val, ddx_val)\n",
    "    \n",
    "    fvu_results_st.append(eval_st)\n",
    "    fvu_results_ptat.append(eval_ptat)\n",
    "\n",
    "# Compute statistics\n",
    "print(\"\\nFVU Results (mean ± std):\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "for name, results in [('ST', fvu_results_st), ('PTAT', fvu_results_ptat)]:\n",
    "    fvu_x = [r['fvu_x'] for r in results]\n",
    "    fvu_ddz = [r['fvu_ddz'] for r in results]\n",
    "    fvu_ddx = [r['fvu_ddx'] for r in results]\n",
    "    \n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  FVU_x:   {np.mean(fvu_x):.4f} ± {np.std(fvu_x):.4f}\")\n",
    "    print(f\"  FVU_ddz: {np.mean(fvu_ddz):.4f} ± {np.std(fvu_ddz):.4f}\")\n",
    "    print(f\"  FVU_ddx: {np.mean(fvu_ddx):.4f} ± {np.std(fvu_ddx):.4f}\")\n",
    "\n",
    "# Visualize FVU distributions\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "\n",
    "metrics = ['fvu_x', 'fvu_ddz', 'fvu_ddx']\n",
    "titles = ['FVU$_x$', 'FVU$_{\\\\ddot{z}}$', 'FVU$_{\\\\ddot{x}}$']\n",
    "\n",
    "for ax, metric, title in zip(axes, metrics, titles):\n",
    "    st_vals = [r[metric] for r in fvu_results_st]\n",
    "    ptat_vals = [r[metric] for r in fvu_results_ptat]\n",
    "    \n",
    "    x_pos = [1, 2]\n",
    "    ax.boxplot([st_vals, ptat_vals], positions=x_pos)\n",
    "    ax.set_xticks(x_pos)\n",
    "    ax.set_xticklabels(['ST', 'PTAT'])\n",
    "    ax.set_ylabel(title)\n",
    "    ax.set_title(f'{title} Distribution')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f512ef",
   "metadata": {},
   "source": [
    "### Analyze Learned Equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614a6ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze learned coefficients across models\n",
    "def print_learned_equation(coefficients, mask, term_names, threshold=0.01):\n",
    "    \"\"\"Format learned equation as string.\"\"\"\n",
    "    active_terms = []\n",
    "    for name, coef, m in zip(term_names, coefficients, mask):\n",
    "        if m and np.abs(coef) > threshold:\n",
    "            if coef > 0:\n",
    "                active_terms.append(f\"+{coef:.4f}·{name}\")\n",
    "            else:\n",
    "                active_terms.append(f\"{coef:.4f}·{name}\")\n",
    "    \n",
    "    if active_terms:\n",
    "        eq = \" \".join(active_terms)\n",
    "        if eq.startswith('+'):\n",
    "            eq = eq[1:]\n",
    "        return f\"ẍ = {eq}\"\n",
    "    else:\n",
    "        return \"ẍ = 0\"\n",
    "\n",
    "\n",
    "print(\"Learned Equations Analysis:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nSequential Thresholding (ST) Results:\")\n",
    "print(\"-\"*50)\n",
    "for i, r in enumerate(all_results_st):\n",
    "    eq = print_learned_equation(r['coefficients'], r['mask'], term_names)\n",
    "    n_active = r['mask'].sum()\n",
    "    print(f\"Model {i+1}: {eq} ({n_active} active terms)\")\n",
    "\n",
    "print(\"\\nPatient Trend-Aware Thresholding (PTAT) Results:\")\n",
    "print(\"-\"*50)\n",
    "for i, r in enumerate(all_results_ptat):\n",
    "    eq = print_learned_equation(r['coefficients'], r['mask'], term_names)\n",
    "    n_active = r['mask'].sum()\n",
    "    print(f\"Model {i+1}: {eq} ({n_active} active terms)\")\n",
    "\n",
    "# Count which terms are most frequently selected\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Term Selection Frequency:\")\n",
    "print(\"-\"*50)\n",
    "\n",
    "st_term_freq = np.zeros(len(term_names))\n",
    "ptat_term_freq = np.zeros(len(term_names))\n",
    "\n",
    "for r in all_results_st:\n",
    "    for i, (coef, mask) in enumerate(zip(r['coefficients'], r['mask'])):\n",
    "        if mask and np.abs(coef) > 0.01:\n",
    "            st_term_freq[i] += 1\n",
    "\n",
    "for r in all_results_ptat:\n",
    "    for i, (coef, mask) in enumerate(zip(r['coefficients'], r['mask'])):\n",
    "        if mask and np.abs(coef) > 0.01:\n",
    "            ptat_term_freq[i] += 1\n",
    "\n",
    "print(f\"\\n{'Term':15s} {'ST':>8s} {'PTAT':>8s}\")\n",
    "print(\"-\"*35)\n",
    "for name, st_f, ptat_f in zip(term_names, st_term_freq, ptat_term_freq):\n",
    "    print(f\"{name:15s} {int(st_f):>8d} {int(ptat_f):>8d}\")\n",
    "\n",
    "# Visualize term selection\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "x = np.arange(len(term_names))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x - width/2, st_term_freq / n_models * 100, width, label='ST', alpha=0.8)\n",
    "bars2 = ax.bar(x + width/2, ptat_term_freq / n_models * 100, width, label='PTAT', alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Term')\n",
    "ax.set_ylabel('Selection Frequency (%)')\n",
    "ax.set_title('Term Selection Frequency Across Models')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(term_names, rotation=45, ha='right')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.axhline(y=50, color='gray', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2adf23d",
   "metadata": {},
   "source": [
    "### Resimulate with Learned Equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764bf8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resimulate_and_decode(sindy_ae_model, coefficients, terms, z0, dz0, T, dt):\n",
    "    \"\"\"\n",
    "    Resimulate with learned coefficients and decode to x-space.\n",
    "    \"\"\"\n",
    "    # Simulate in z-space\n",
    "    t, z_sim, dz_sim, ddz_sim = simulate_pendulum(z0, dz0, coefficients, terms, T, dt)\n",
    "    \n",
    "    # Embed ground truth to x-space\n",
    "    x_gt, dx_gt, ddx_gt = embed_cartesian(z_sim, dz_sim, ddz_sim)\n",
    "    \n",
    "    # Decode using trained decoder\n",
    "    sindy_ae_model.eval()\n",
    "    with torch.no_grad():\n",
    "        z_t = torch.tensor(z_sim.reshape(-1, 1), dtype=torch.float32).to(device)\n",
    "        x_decoded = sindy_ae_model.decode(z_t).cpu().numpy()\n",
    "    \n",
    "    return t, z_sim, x_gt, x_decoded\n",
    "\n",
    "\n",
    "# Test resimulation with one of the best models\n",
    "best_st_idx = np.argmin([r['fvu_x'] for r in fvu_results_st])\n",
    "best_ptat_idx = np.argmin([r['fvu_x'] for r in fvu_results_ptat])\n",
    "\n",
    "best_st_model = all_results_st[best_st_idx]['model']\n",
    "best_ptat_model = all_results_ptat[best_ptat_idx]['model']\n",
    "\n",
    "# Test initial conditions\n",
    "test_ics = [(0.5, 0.0), (1.5, 0.5), (2.5, -0.3)]\n",
    "\n",
    "fig, axes = plt.subplots(len(test_ics), 4, figsize=(16, 4*len(test_ics)))\n",
    "\n",
    "for row, (z0, dz0) in enumerate(test_ics):\n",
    "    # Ground truth\n",
    "    t, z_gt, dz_gt, ddz_gt = simulate_pendulum(\n",
    "        z0, dz0, GROUND_TRUTH_COEFFICIENTS, terms, 50, 0.02\n",
    "    )\n",
    "    x_gt, _, _ = embed_cartesian(z_gt, dz_gt, ddz_gt)\n",
    "    \n",
    "    # Resimulate with ST model\n",
    "    t_st, z_st, x_gt_st, x_dec_st = resimulate_and_decode(\n",
    "        best_st_model, all_results_st[best_st_idx]['coefficients'],\n",
    "        terms, z0, dz0, 50, 0.02\n",
    "    )\n",
    "    \n",
    "    # Resimulate with PTAT model\n",
    "    t_ptat, z_ptat, x_gt_ptat, x_dec_ptat = resimulate_and_decode(\n",
    "        best_ptat_model, all_results_ptat[best_ptat_idx]['coefficients'],\n",
    "        terms, z0, dz0, 50, 0.02\n",
    "    )\n",
    "    \n",
    "    # Plot z trajectory\n",
    "    axes[row, 0].plot(t, z_gt, 'k-', label='Ground Truth', linewidth=2)\n",
    "    axes[row, 0].plot(t_st, z_st, 'b--', label='ST', alpha=0.8)\n",
    "    axes[row, 0].plot(t_ptat, z_ptat, 'r-.', label='PTAT', alpha=0.8)\n",
    "    axes[row, 0].set_xlabel('Time (s)')\n",
    "    axes[row, 0].set_ylabel('z')\n",
    "    axes[row, 0].set_title(f'z(t): z₀={z0}, ż₀={dz0}')\n",
    "    axes[row, 0].legend()\n",
    "    axes[row, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot x trajectory\n",
    "    axes[row, 1].plot(x_gt[:, 0], x_gt[:, 1], 'k-', label='Ground Truth', linewidth=2)\n",
    "    axes[row, 1].plot(x_dec_st[:, 0], x_dec_st[:, 1], 'b--', label='ST Decoded', alpha=0.8)\n",
    "    axes[row, 1].plot(x_dec_ptat[:, 0], x_dec_ptat[:, 1], 'r-.', label='PTAT Decoded', alpha=0.8)\n",
    "    axes[row, 1].set_xlabel('x₁')\n",
    "    axes[row, 1].set_ylabel('x₂')\n",
    "    axes[row, 1].set_title('x(t) Decoded from z')\n",
    "    axes[row, 1].legend()\n",
    "    axes[row, 1].grid(True, alpha=0.3)\n",
    "    axes[row, 1].set_aspect('equal')\n",
    "    \n",
    "    # Plot z error over time\n",
    "    z_error_st = np.abs(z_st - z_gt)\n",
    "    z_error_ptat = np.abs(z_ptat - z_gt)\n",
    "    \n",
    "    axes[row, 2].semilogy(t, z_error_st, 'b-', label='ST')\n",
    "    axes[row, 2].semilogy(t, z_error_ptat, 'r-', label='PTAT')\n",
    "    axes[row, 2].axvline(x=1.0, color='gray', linestyle='--', alpha=0.5)\n",
    "    axes[row, 2].set_xlabel('Time (s)')\n",
    "    axes[row, 2].set_ylabel('|z - ẑ|')\n",
    "    axes[row, 2].set_title('z Error over Time')\n",
    "    axes[row, 2].legend()\n",
    "    axes[row, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot x error over time\n",
    "    x_error_st = np.sqrt(np.sum((x_gt - x_dec_st)**2, axis=1))\n",
    "    x_error_ptat = np.sqrt(np.sum((x_gt - x_dec_ptat)**2, axis=1))\n",
    "    \n",
    "    axes[row, 3].semilogy(t, x_error_st, 'b-', label='ST')\n",
    "    axes[row, 3].semilogy(t, x_error_ptat, 'r-', label='PTAT')\n",
    "    axes[row, 3].axvline(x=1.0, color='gray', linestyle='--', alpha=0.5)\n",
    "    axes[row, 3].set_xlabel('Time (s)')\n",
    "    axes[row, 3].set_ylabel('||x - x̂||')\n",
    "    axes[row, 3].set_title('x Error over Time')\n",
    "    axes[row, 3].legend()\n",
    "    axes[row, 3].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Note: Up to t=1s, resimulation should be very close to ground truth.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bb2b2b",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 3: Bonus - SINDy-Autoencoder on Videos\n",
    "\n",
    "## 3.1 Artificial Video Embedding\n",
    "\n",
    "Create video frames with a Gaussian peak at the pendulum tip location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf8c6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_grid(z, t, resolution=32, sigma=0.1):\n",
    "    \"\"\"\n",
    "    Create video of a Gaussian peak at pendulum tip.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    z : ndarray\n",
    "        Angle trajectory, shape (N, T) or (T,)\n",
    "    t : ndarray\n",
    "        Time array\n",
    "    resolution : int\n",
    "        Video frame resolution (resolution x resolution)\n",
    "    sigma : float\n",
    "        Gaussian standard deviation\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    x : ndarray\n",
    "        Video frames, shape (*z.shape, resolution*resolution)\n",
    "    dx : ndarray\n",
    "        Time derivative (finite difference)\n",
    "    ddx : ndarray\n",
    "        Second time derivative (finite difference)\n",
    "    \"\"\"\n",
    "    z = np.atleast_2d(z)  # Shape: (N, T)\n",
    "    N, T_steps = z.shape\n",
    "    \n",
    "    # Compute Cartesian coordinates of tip\n",
    "    tip_x = np.sin(z)  # Shape: (N, T)\n",
    "    tip_y = -np.cos(z)\n",
    "    \n",
    "    # Create pixel grid [-1.5, 1.5]\n",
    "    pixel_coords = np.linspace(-1.5, 1.5, resolution)\n",
    "    xx, yy = np.meshgrid(pixel_coords, pixel_coords)\n",
    "    \n",
    "    # Flatten grid for easier computation\n",
    "    xx_flat = xx.flatten()  # Shape: (resolution*resolution,)\n",
    "    yy_flat = yy.flatten()\n",
    "    \n",
    "    # Create video frames\n",
    "    video = np.zeros((N, T_steps, resolution * resolution))\n",
    "    \n",
    "    for n in range(N):\n",
    "        for t_idx in range(T_steps):\n",
    "            # Compute Gaussian\n",
    "            dx = xx_flat - tip_x[n, t_idx]\n",
    "            dy = yy_flat - tip_y[n, t_idx]\n",
    "            dist_sq = dx**2 + dy**2\n",
    "            gaussian = np.exp(-dist_sq / (2 * sigma**2))\n",
    "            video[n, t_idx] = gaussian\n",
    "    \n",
    "    # Compute derivatives using finite differences\n",
    "    dt = t[1] - t[0] if len(t) > 1 else 0.02\n",
    "    \n",
    "    # First derivative: dx[t] = (x[t+1] - x[t-1]) / (2*dt)\n",
    "    dx_video = np.zeros_like(video)\n",
    "    dx_video[:, 1:-1] = (video[:, 2:] - video[:, :-2]) / (2 * dt)\n",
    "    dx_video[:, 0] = (video[:, 1] - video[:, 0]) / dt\n",
    "    dx_video[:, -1] = (video[:, -1] - video[:, -2]) / dt\n",
    "    \n",
    "    # Second derivative: ddx[t] = (x[t-1] - 2*x[t] + x[t+1]) / dt²\n",
    "    ddx_video = np.zeros_like(video)\n",
    "    ddx_video[:, 1:-1] = (video[:, :-2] - 2*video[:, 1:-1] + video[:, 2:]) / (dt**2)\n",
    "    ddx_video[:, 0] = ddx_video[:, 1]\n",
    "    ddx_video[:, -1] = ddx_video[:, -2]\n",
    "    \n",
    "    return video, dx_video, ddx_video\n",
    "\n",
    "\n",
    "# Create video embedding for a sample trajectory\n",
    "sample_z = train_data['z'][0:5]  # 5 trajectories\n",
    "sample_t = train_data['t'][0]\n",
    "\n",
    "video, dvideo, ddvideo = embed_grid(sample_z, sample_t, resolution=32, sigma=0.1)\n",
    "\n",
    "print(f\"Video embedding shapes:\")\n",
    "print(f\"  Video: {video.shape}\")\n",
    "print(f\"  dVideo: {dvideo.shape}\")\n",
    "print(f\"  ddVideo: {ddvideo.shape}\")\n",
    "\n",
    "# Visualize video frames\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "\n",
    "# First trajectory\n",
    "traj_idx = 0\n",
    "frames_to_show = [0, 10, 20, 30, 40]\n",
    "\n",
    "for col, t_idx in enumerate(frames_to_show):\n",
    "    frame = video[traj_idx, t_idx].reshape(32, 32)\n",
    "    axes[0, col].imshow(frame, extent=[-1.5, 1.5, -1.5, 1.5], origin='lower', cmap='hot')\n",
    "    axes[0, col].set_title(f't = {t_idx * 0.02:.2f}s')\n",
    "    axes[0, col].set_xlabel('x₁')\n",
    "    if col == 0:\n",
    "        axes[0, col].set_ylabel('x₂')\n",
    "    \n",
    "    # Also show derivative frame\n",
    "    dframe = dvideo[traj_idx, t_idx].reshape(32, 32)\n",
    "    im = axes[1, col].imshow(dframe, extent=[-1.5, 1.5, -1.5, 1.5], origin='lower', cmap='RdBu')\n",
    "    if col == 0:\n",
    "        axes[1, col].set_ylabel('x₂')\n",
    "\n",
    "axes[1, 2].set_xlabel('Video Derivative')\n",
    "\n",
    "plt.suptitle('Video Embedding: Gaussian Peak at Pendulum Tip', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc33683",
   "metadata": {},
   "source": [
    "## 3.2-3.3 SINDy-Autoencoder on Videos\n",
    "\n",
    "Use encoder hidden layers [128, 64, 32] and decoder [32, 64, 128] as in Champion et al."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab15cd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create video data for training\n",
    "print(\"Creating video training data...\")\n",
    "video_train, dvideo_train, ddvideo_train = embed_grid(\n",
    "    train_data['z'], train_data['t'][0], resolution=32, sigma=0.1\n",
    ")\n",
    "\n",
    "# Flatten and reshape\n",
    "video_flat = video_train.reshape(-1, 32*32)  # Shape: (N*T, 1024)\n",
    "dvideo_flat = dvideo_train.reshape(-1, 32*32)\n",
    "ddvideo_flat = ddvideo_train.reshape(-1, 32*32)\n",
    "\n",
    "print(f\"Video data shape: {video_flat.shape}\")\n",
    "\n",
    "# Split data\n",
    "vid_tr, vid_vl, dvid_tr, dvid_vl, ddvid_tr, ddvid_vl = train_test_split(\n",
    "    video_flat, dvideo_flat, ddvideo_flat, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "train_ds_vid = TensorDataset(\n",
    "    torch.tensor(vid_tr, dtype=torch.float32),\n",
    "    torch.tensor(dvid_tr, dtype=torch.float32),\n",
    "    torch.tensor(ddvid_tr, dtype=torch.float32)\n",
    ")\n",
    "val_ds_vid = TensorDataset(\n",
    "    torch.tensor(vid_vl, dtype=torch.float32),\n",
    "    torch.tensor(dvid_vl, dtype=torch.float32),\n",
    "    torch.tensor(ddvid_vl, dtype=torch.float32)\n",
    ")\n",
    "\n",
    "train_loader_vid = DataLoader(train_ds_vid, batch_size=256, shuffle=True)\n",
    "val_loader_vid = DataLoader(val_ds_vid, batch_size=256, shuffle=False)\n",
    "\n",
    "print(f\"Video dataset: {len(train_ds_vid)} train, {len(val_ds_vid)} val\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62dc37d1",
   "metadata": {},
   "source": [
    "### Train SINDy-Autoencoder on Video Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6ffe4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train SINDy-Autoencoder on video with ST\n",
    "print(\"Training SINDy-Autoencoder on Video (Sequential Thresholding)...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "sindy_ae_vid_st = SINDyAutoencoder(\n",
    "    input_dim=32*32,  # Flattened video\n",
    "    latent_dim=1,\n",
    "    encoder_hidden=[128, 64, 32],\n",
    "    decoder_hidden=[32, 64, 128],\n",
    "    n_sindy_terms=10\n",
    ").to(device)\n",
    "\n",
    "history_vid_st = train_sindy_autoencoder(\n",
    "    sindy_ae_vid_st, train_loader_vid, val_loader_vid,\n",
    "    n_epochs=6000,\n",
    "    lr=1e-3,\n",
    "    lambda_ddz=5e-5, lambda_ddx=5e-4, lambda_l1=1e-5,\n",
    "    thresholding='sequential',\n",
    "    threshold_a=0.1, threshold_interval=500,\n",
    "    refinement_epoch=5000,\n",
    "    verbose=True, log_interval=500\n",
    ")\n",
    "\n",
    "vid_st_coeffs = sindy_ae_vid_st.get_sindy_coefficients()\n",
    "vid_st_mask = sindy_ae_vid_st.get_sindy_mask()\n",
    "\n",
    "print(\"\\nVideo ST Results:\")\n",
    "print(\"-\"*50)\n",
    "for name, coef, mask in zip(term_names, vid_st_coeffs, vid_st_mask):\n",
    "    if mask:\n",
    "        print(f\"  {name:15s}: {coef:10.6f} [Active]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573fd7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train SINDy-Autoencoder on video with PTAT\n",
    "print(\"Training SINDy-Autoencoder on Video (PTAT)...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "sindy_ae_vid_ptat = SINDyAutoencoder(\n",
    "    input_dim=32*32,\n",
    "    latent_dim=1,\n",
    "    encoder_hidden=[128, 64, 32],\n",
    "    decoder_hidden=[32, 64, 128],\n",
    "    n_sindy_terms=10\n",
    ").to(device)\n",
    "\n",
    "history_vid_ptat = train_sindy_autoencoder(\n",
    "    sindy_ae_vid_ptat, train_loader_vid, val_loader_vid,\n",
    "    n_epochs=6000,\n",
    "    lr=1e-3,\n",
    "    lambda_ddz=5e-5, lambda_ddx=5e-4, lambda_l1=1e-5,\n",
    "    thresholding='patient',\n",
    "    threshold_a=0.1, threshold_b=0.002, patience=1000,\n",
    "    refinement_epoch=5000,\n",
    "    verbose=True, log_interval=500\n",
    ")\n",
    "\n",
    "vid_ptat_coeffs = sindy_ae_vid_ptat.get_sindy_coefficients()\n",
    "vid_ptat_mask = sindy_ae_vid_ptat.get_sindy_mask()\n",
    "\n",
    "print(\"\\nVideo PTAT Results:\")\n",
    "print(\"-\"*50)\n",
    "for name, coef, mask in zip(term_names, vid_ptat_coeffs, vid_ptat_mask):\n",
    "    if mask:\n",
    "        print(f\"  {name:15s}: {coef:10.6f} [Active]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a228cc04",
   "metadata": {},
   "source": [
    "## 3.4 Evaluation: Compare ST vs PTAT on Videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b56a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare ST vs PTAT on video data\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Plot training histories\n",
    "plot_sindy_ae_history(history_vid_st, \"Video ST\", term_names, refinement_epoch=5000)\n",
    "plot_sindy_ae_history(history_vid_ptat, \"Video PTAT\", term_names, refinement_epoch=5000)\n",
    "\n",
    "# Summary comparison\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Video Data: ST vs PTAT Comparison\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nSequential Thresholding (ST):\")\n",
    "st_eq = print_learned_equation(vid_st_coeffs, vid_st_mask, term_names)\n",
    "print(f\"  Equation: {st_eq}\")\n",
    "print(f\"  Active terms: {vid_st_mask.sum()}\")\n",
    "\n",
    "print(\"\\nPatient Trend-Aware Thresholding (PTAT):\")\n",
    "ptat_eq = print_learned_equation(vid_ptat_coeffs, vid_ptat_mask, term_names)\n",
    "print(f\"  Equation: {ptat_eq}\")\n",
    "print(f\"  Active terms: {vid_ptat_mask.sum()}\")\n",
    "\n",
    "# Compare coefficients\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "x_pos = np.arange(len(term_names))\n",
    "width = 0.35\n",
    "\n",
    "# ST coefficients\n",
    "colors_st = ['green' if vid_st_mask[i] else 'gray' for i in range(len(term_names))]\n",
    "axes[0].bar(x_pos, vid_st_coeffs, color=colors_st)\n",
    "axes[0].set_xticks(x_pos)\n",
    "axes[0].set_xticklabels(term_names, rotation=45, ha='right')\n",
    "axes[0].set_ylabel('Coefficient')\n",
    "axes[0].set_title('Video ST - Final Coefficients')\n",
    "axes[0].axhline(y=0, color='k', linestyle='-', linewidth=0.5)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# PTAT coefficients\n",
    "colors_ptat = ['green' if vid_ptat_mask[i] else 'gray' for i in range(len(term_names))]\n",
    "axes[1].bar(x_pos, vid_ptat_coeffs, color=colors_ptat)\n",
    "axes[1].set_xticks(x_pos)\n",
    "axes[1].set_xticklabels(term_names, rotation=45, ha='right')\n",
    "axes[1].set_ylabel('Coefficient')\n",
    "axes[1].set_title('Video PTAT - Final Coefficients')\n",
    "axes[1].axhline(y=0, color='k', linestyle='-', linewidth=0.5)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CONCLUSION\")\n",
    "print(\"=\"*60)\n",
    "print(\"\"\"\n",
    "For high-dimensional data like videos:\n",
    "- Sequential Thresholding (ST) may select incorrect or too many terms because\n",
    "  it makes hard thresholding decisions at fixed intervals without considering\n",
    "  the trend of coefficient changes.\n",
    "  \n",
    "- Patient Trend-Aware Thresholding (PTAT) is more robust because it:\n",
    "  1. Monitors coefficient changes over time\n",
    "  2. Only removes terms that have consistently small values AND small changes\n",
    "  3. Uses a patience mechanism to avoid premature pruning\n",
    "\n",
    "This demonstrates that while ST may work well for simpler problems (like \n",
    "Cartesian coordinates), more sophisticated thresholding like PTAT is needed\n",
    "for challenging high-dimensional settings where the optimization landscape\n",
    "is more complex.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac1d80d",
   "metadata": {},
   "source": [
    "---\n",
    "# Summary\n",
    "\n",
    "This notebook implemented the SINDy (Sparse Identification of Nonlinear Dynamics) algorithm for symbolic regression:\n",
    "\n",
    "## Key Accomplishments:\n",
    "\n",
    "### Part 1: SINDy in Ground Truth Coordinates\n",
    "- Implemented pendulum simulation with ground truth ODE: $\\ddot{z} = -\\sin(z)$\n",
    "- Created SINDy library: $\\Theta(z, \\dot{z}) = [1, z, \\dot{z}, \\sin(z), z^2, ...]$\n",
    "- Implemented LASSO regression using sklearn and PyTorch\n",
    "- Developed Sequential Thresholding (ST) and Patient Trend-Aware Thresholding (PTAT)\n",
    "- Demonstrated small angle approximation: $\\sin(z) \\approx z$\n",
    "\n",
    "### Part 2: SINDy-Autoencoder\n",
    "- Embedded pendulum data into 2D Cartesian coordinates\n",
    "- Built autoencoder with derivative propagation through layers\n",
    "- Implemented `SigmoidDerivatives` and `LinearDerivatives` layers\n",
    "- Created `SINDyAutoencoder` combining representation learning with equation discovery\n",
    "- Trained with refinement phase (L1 regularization → no regularization)\n",
    "- Evaluated using Fraction of Variance Unexplained (FVU)\n",
    "\n",
    "### Part 3: Bonus - Video Data\n",
    "- Created artificial video embedding with Gaussian peaks\n",
    "- Trained deeper networks for high-dimensional video input\n",
    "- Demonstrated that PTAT is more reliable than ST for complex data\n",
    "\n",
    "## Key Findings:\n",
    "1. SINDy can correctly identify the pendulum equation $\\ddot{z} = -\\sin(z)$\n",
    "2. Thresholding is essential for sparse coefficient selection\n",
    "3. PTAT provides more robust coefficient selection than ST, especially for high-dimensional data\n",
    "4. The autoencoder successfully learns to encode Cartesian/video data to the canonical angle coordinate"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
